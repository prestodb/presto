GENERIC_USER_ERROR_FUNCTION_LANGUAGE_NOT_SUPPORTED=Catalog %s does not support functions implemented in language %s
GENERIC_USER_ERROR_FUNCTION_ID_MISSING=Function '%s' is missing from cache
GENERIC_USER_ERROR_FUNCTION_ID_ALREADY_EXISTS=Function '%s' already exists
GENERIC_USER_ERROR_FUNCTION_PARAMETER_COUNT_EXCEEDS=Function has more than %s parameters: %s
GENERIC_USER_ERROR_FIELD_LENGTH_EXCEEDS_MAX=%s exceeds max length of %s: %s
GENERIC_USER_ERROR_COMPRESSION_NOT_SUPPORTED=%s compression is not supported with %s
GENERIC_USER_ERROR_TABLE_PROPERTY_WITH_PARTITION=Table property %s is only allowed if there is also %s
GENERIC_USER_ERROR_MISSING_TABLE_PROPERTY=Missing table property %s
GENERIC_USER_ERROR_CREATE_SCHEMA_IN_KUDU_CONNECTOR_NOT_ALLOWED=Creating schema in Kudu connector not allowed if schema emulation is disabled.
GENERIC_USER_ERROR_DELETING_DEFAULT_SCHEMA_NOT_ALLOWED=Deleting default schema not allowed.
GENERIC_USER_ERROR_TABLE_NAME_CONFLICT_NO_DOTS_ALLOWED_IN_SCHEMA=Table name conflicts with schema emulation settings. No '.' allowed for tables in schema 'default'.
GENERIC_USER_ERROR_TABLE_NAME_MUST_NOT_START_WITH_COMMON_PREFIX=Table name conflicts with schema emulation settings. Table name must not start with %s .
GENERIC_USER_ERROR_CANNOT_CREATE_FUNCTION_IN_BUILDIN_FUNCTION=Cannot create function in built-in function namespace: %s
GENERIC_USER_ERROR_CANNOT_ALTER_FUNCTION_IN_BUILDIN_FUNCTION=Cannot alter function in built-in function namespace: %s
GENERIC_USER_ERROR_CANNOT_DROP_FUNCTION_IN_BUILDIN_FUNCTION=Cannot drop function in built-in function namespace: %s
GENERIC_USER_ERROR_CANNOT_CREATE_FUNCTION_IN_FUNCTION=Cannot create function in function namespace: %s
GENERIC_USER_ERROR_REGEXP_MATCHING_INTERRUPTED=Regexp matching interrupted
GENERIC_USER_ERROR_NOT_A_VALID_TIMESTAMP_LITERAL=%s' is not a valid timestamp literal
GENERIC_USER_ERROR_REMOTE_FUNCTIONS_NOT_ENABLED=Remote functions are not enabled
GENERIC_USER_ERROR_ONLY_SUPPORTED_FOR_SQL_QUERY=SHOW CREATE FUNCTION is only supported for SQL functions
GENERIC_USER_ERROR_INVALID_PATTERN_PASSED=invalid joda pattern '%s' passed as format hint for column '%s'
SYNTAX_ERROR_CANNOT_RENAME_TABLE=Cannot rename tables across catalogs
SYNTAX_ERROR_TOO_MANY_DOTS_IN_TABLE_NAME=Too many dots in table name: %s
ABANDONED_QUERY_QUERY_NOT_ACCESSED=Query %s has not been accessed since %s: currentTime %s
USER_CANCELED_QUERY_CANCELED=Query was canceled
PERMISSION_DENIED_DROP_TABLE_DISABLED_IN_CATALOG=DROP TABLE is disabled in this catalog
PERMISSION_DENIED_DROP_TABLE_DISABLED_IN_CASSANDRA=DROP TABLE is disabled in this Cassandra catalog
PERMISSION_DENIED_USER_DOES_NOT_HAVE_ACCESS=User does not have access to encryption key for encrypted column = %s. If returning 'null' for encrypted columns is acceptable to your query, please add 'set session hive.read_null_masked_parquet_encrypted_value_enabled=true' before your query
PERMISSION_DENIED_ACCESS_DENIED=Access Denied: %s
NOT_FOUND_FAILED_TO_FIND_COLUMN=Failed to find source column %s to rename to %s
NOT_FOUND_FAILED_TO_FACTORY_SERIALIZER_CLASS=Failed to factory serializer class.  Is it on the classpath?
NOT_FOUND_SERIALIZER_CLASS_NOT_FOUND=Configured serializer class not found
NOT_FOUND_NO_COLUMN=No column with name %s
NOT_FOUND_PREPARED_STATEMENT_NOT_FOUND=Prepared statement not found: %s
NOT_FOUND_SCHEMA_NOT_FOUND=Schema not found: %s
NOT_FOUND_SCHEMA=Schema schema1 not found
NOT_FOUND_TABLE_NO_LONGER_EXIST=Delta table (%s.%s) no longer exists.
NOT_FOUND_SNAPSHOT_VERSION_DOES_NOT_EXIST=Snapshot version %d does not exist in Delta table '%s'.
NOT_FOUND_CREATED_SNAPSHOT_DOES_NOT_EXIST=There is no snapshot exists in Delta table '%s' that is created on or before '%s'
NOT_FOUND_TYPE_NOT_FOUND=Type %s not found
NOT_FOUND_FUNCTION_NAMESPACE_NOT_FOUND=Function namespace not found: %s
NOT_FOUND_FUNCTION_NOT_FOUND=Function not found: %s%s
NOT_FOUND_TABLE_HANDLE=Hive table handle not found
NOT_FOUND_TABLE_DOES_NOT_HAVE_COLUMN=Table %s.%s does not have columns %s
NOT_FOUND_MISSING_ENTRY_FOR_DATABASES=Missing entry for all databases
NOT_FOUND_MISSING_ENTRY_FOR_ROLES=Missing entry for roles
NOT_FOUND_MISSING_ENTRY_FOR_KEYS=Missing entry found for key: %s
NOT_FOUND_SESSION_PROPERTY_CATALOG=Session property catalog does not exist: %s
NOT_FOUND_CATALOG_DOES_NOT_EXIST=Catalog does not exist: %s
NOT_FOUND_TARGET_QUERY=Target query not found: %s
NOT_FOUND_TABLE=Table %s not found
NOT_FOUND_SESSION_FUNCTION=Session function %s not found
NOT_FOUND_CATALOG=Catalog not found: %s
NOT_FOUND_SCHEMA_DOES_NOT_EXIT=Schema [%s] does not exist
NOT_FOUND_IS_EMPTY=%s is empty
FUNCTION_NOT_FOUND_NOT_REGISTERED=Function %s not registered
FUNCTION_NOT_FOUND_FUNCTION_DOES_NOT_EXIST=Function not found: %s
FUNCTION_NOT_FOUND_INCORRECT_REFERENCE=Functions that are not temporary or builtin must be referenced by 'catalog.schema.function_name', found: %s
FUNCTION_NOT_FOUND_DEPENDENT_FUNCTION_NOT_AVAILABLE=Dependent function implementation (%s) with convention (%s) is not available
FUNCTION_NOT_FOUND_FUNCTION_CANNOT_BE_RESOLVED=Sampling function: %s not cannot be resolved
FUNCTION_NOT_FOUND_FUNCTION_OF_TYPE=Function not found: %s%s
INVALID_FUNCTION_ARGUMENT_COLUMN_CANNOT_BE_NULL=Column mapped as the Accumulo row ID cannot be null
INVALID_FUNCTION_ARGUMENT_NUMBER_NOT_EQUAL_TO_LENGTH="Number of split tokens is not equal to schema length. Expected %s received %s. Schema: %s, fields {%s}, delimiter %s"
INVALID_FUNCTION_ARGUMENT_INVALID_GEOJSON=Invalid GeoJSON: %s
INVALID_FUNCTION_ARGUMENT_INVALID_WKT=Invalid WKT: %s
INVALID_FUNCTION_ARGUMENT_INVALID_GEOMETRY_TYPE=Geometry type not valid
INVALID_FUNCTION_ARGUMENT_UNKNOWN_GEOMETRY_TYPE=Unknown geometry type: %s
INVALID_FUNCTION_ARGUMENT_LATITUDE_NOT_IN_RANGE=Latitude must be between -90 and 90
INVALID_FUNCTION_ARGUMENT_LONGITUDE_NOT_IN_RANGE=Longitude must be between -180 and 180
INVALID_FUNCTION_ARGUMENT_NOT_VALID_TYPE=When applied to SphericalGeography inputs, %s only supports %s. Input type is: %s
INVALID_FUNCTION_ARGUMENT_MAP_KEY_NULL=map key cannot be null
INVALID_FUNCTION_ARGUMENT_INVALID_QUADKEY_SEQUENCE=Invalid QuadKey digit sequence: %s
INVALID_FUNCTION_ARGUMENT_INVALID_INPUT_NULL_AT_INDEX=Invalid input to %s: null at index %s
INVALID_FUNCTION_ARGUMENT_INVALID_INPUT=Invalid input to %s: geometry is not a point: %s at index %s
INVALID_FUNCTION_ARGUMENT_EMPTY_POINT_AT_INDEX=Invalid input to %s: empty point at index %s
INVALID_FUNCTION_ARGUMENT_CONSECUTIVE_DUPLICATE_POINTS_AT_INDEX=Invalid input to %s: consecutive duplicate points at index %s
INVALID_FUNCTION_ARGUMENT_INVALID_GEOMETRY=Invalid geometry: %s
INVALID_FUNCTION_ARGUMENT_DISTANCE_NAN=distance is NaN
INVALID_FUNCTION_ARGUMENT_DISTANCE_NEGATIVE=distance is negative
INVALID_FUNCTION_ARGUMENT_INVALID_TYPE=Invalid type for isClosed: %s
INVALID_FUNCTION_ARGUMENT_NOT_LINESTRING_OR_MULTISTRING=First argument to line_locate_point must be a LineString or a MultiLineString. Got: %s
INVALID_FUNCTION_ARGUMENT_NOT_A_POINT=Second argument to line_locate_point must be a Point. Got: %s
INVALID_FUNCTION_ARGUMENT_FRACTION_NOT_BETWEEN_0_AND_1=line_interpolate_point: Fraction must be between 0 and 1, but is %s
INVALID_FUNCTION_ARGUMENT_DISTANCE_TOLERANCE_IS_NAN=distanceTolerance is NaN
INVALID_FUNCTION_ARGUMENT_DISTANCE_TOLERANCE_IS_NEGATIVE=distanceTolerance is negative
INVALID_FUNCTION_ARGUMENT_EXPAND_ENVELOPE_DISTANCE_IS_NAN=expand_envelope: distance is NaN
INVALID_FUNCTION_ARGUMENT_EXPAND_ENVELOPE_DISTANCE_IS_NEGATIVE=expand_envelope: distance %s is negative
INVALID_FUNCTION_ARGUMENT_DISTANCE_INFINITE=distance is infinite
INVALID_FUNCTION_ARGUMENT_INVALID_WKB=Invalid WKB
INVALID_FUNCTION_ARGUMENT_DOES_NOT_APPLY=%s only applies to %s. Input type is: %s
INVALID_FUNCTION_ARGUMENT_NO_ROWS_SUPPLIED=No rows supplied to spatial partition.
INVALID_FUNCTION_ARGUMENT_CANNOT_CONVERT_3D_GEOMETRY=Cannot convert 3D geometry to a spherical geography
INVALID_FUNCTION_ARGUMENT_CANNOT_CONVERT_GEOMETRY=Cannot convert geometry of this type to spherical geography: %s
INVALID_FUNCTION_ARGUMENT_AVERAGE_VECTOR_LENGTH=Unexpected error. Average vector length adds to zero (%f, %f, %f)
INVALID_FUNCTION_ARGUMENT_POLYGON_NOT_VALID=Polygon is not valid: a loop contains less then 3 vertices.
INVALID_FUNCTION_ARGUMENT_POLYGON_CANNOT_HAVE_TWO_IDENTICAL_VERTICES=Polygon is not valid: it has two identical consecutive vertices
INVALID_FUNCTION_ARGUMENT_INVALID_INPUT_TO_ST_MUTIPOINT=Invalid input to ST_MultiPoint: %s
INVALID_FUNCTION_ARGUMENT_SECOND_ARGUMENT_N0T_POSITIVE=second argument of max_n/min_n must be positive
INVALID_FUNCTION_ARGUMENT_SECOND_ARGUMENT_NOT_MORE_THAN_MAX_NUMBER_OF_VALUES=second argument of max_n/min_n must be less than or equal to %s; found %s
INVALID_FUNCTION_ARGUMENT_COUNT_ARGUMENT_NOT_CONSTANT=Count argument is not constant: found multiple values [%s, %s]
INVALID_FUNCTION_ARGUMENT_PERCENTILE_NOT_IN_RANGE=Percentile must be between 0 and 1
INVALID_FUNCTION_ARGUMENT_PERCENTILE_ACCURACY_NOT_IN_RANGE=Percentile accuracy must be strictly between 0 and 1
INVALID_FUNCTION_ARGUMENT_PERCENTILE_WEIGHT_NOT_GREATER_THAN_ZERO=percentile weight must be > 0
INVALID_FUNCTION_ARGUMENT_PERCENTILE_IS_NULL=Percentile cannot be null
INVALID_FUNCTION_ARGUMENT_HISTOGRAM_BUCKET_COUNT_NOT_GREATER_THAN_ONE=numeric_histogram bucket count must be greater than one
INVALID_FUNCTION_ARGUMENT_ENTROPY_COUNT_ARGUMENT_IS_NEGATIVE=Entropy count argument must be non-negative
INVALID_FUNCTION_ARGUMENT_STANDARD_ERROR_NOT_IN_RANGE=Max standard error must be in [%s, %s]: %s
INVALID_FUNCTION_ARGUMENT_ADDED_A_DOUBLE_TO_Q-DIGEST=cannot add a double to a q-digest
INVALID_FUNCTION_ARGUMENT_ADDED_A_LONG_TO_T-DIGEST=cannot add a long to a t-digest
INVALID_FUNCTION_ARGUMENT_UNSUPPORTED_COUNT_OF_ARGUMENTS=Unsupported number of arguments: %s
INVALID_FUNCTION_ARGUMENT_NOT_A_STATISTICAL_DIGEST=%s must be a statistical digest
INVALID_FUNCTION_ARGUMENT_SUPPLIED_UNSUPPORTED_TYPE=Unsupported type %s supplied
INVALID_FUNCTION_ARGUMENT_WEIGHT_NOT_GREATER_THAN_ZERO=Weight must be > 0, was %s
INVALID_FUNCTION_ARGUMENT_COMPRESSION_FACTOR_NEGATIVE=Compression factor must be positive, was %s,
INVALID_FUNCTION_ARGUMENT_BUCKET_COUNT_NOT_GREATER_THAN_ONE=approx_most_frequent bucket count must be greater than one, input bucket count: %s
INVALID_FUNCTION_ARGUMENT_INVALID_METHOD_IN_DIFFERENTIAL_ENTROPY=In differential_entropy UDF, invalid method: %s
INVALID_FUNCTION_ARGUMENT_STRATEGY_CLASS_NOT_COMPATIBLE=In differential_entropy, strategy class is not compatible with entropy method: %s %s
INVALID_FUNCTION_ARGUMENT_UNKNOWN_ENTROPY_METHOD=In differential_entropy, unknown entropy method: %s
INVALID_FUNCTION_ARGUMENT_BUCKET_COUNT_NEGATIVE=In differential_entropy UDF, bucket count must be non-negative: %s
INVALID_FUNCTION_ARGUMENT_MIN_NOT_LARGER_THAN_MAX=In differential_entropy UDF, min must be larger than max: min=%s, max=%s
INVALID_FUNCTION_ARGUMENT_WEIGHT_NEGATIVE_IN_UDF=In differential_entropy UDF, weight must be non-negative: %s
INVALID_FUNCTION_ARGUMENT_INCONSISTENT_BUCKET_COUNT=In differential_entropy UDF, inconsistent bucket count: prev=%s, current=%s
INVALID_FUNCTION_ARGUMENT_INCONSISTENT_MIN=In differential_entropy UDF, inconsistent min: prev=%s, current=%s
INVALID_FUNCTION_ARGUMENT_INCONSISTENT_MAX=In differential_entropy UDF, inconsistent max: prev=%s, current=%s
INVALID_FUNCTION_ARGUMENT_SAMPLE_LESS_THAN_MIN=In differential_entropy UDF, sample must be at least min: sample=%s, min=%s
INVALID_FUNCTION_ARGUMENT_SAMPLE_GREATER_THAN_MAX=In differential_entropy UDF, sample must be at most max: sample=%s, max=%s
INVALID_FUNCTION_ARGUMENT_MAX_SAMPLE_NEGATIVE=In differential_entropy UDF, max samples must be positive: %s
INVALID_FUNCTION_ARGUMENT_MAX_SAMPLE_NOT_CAPPED=In differential_entropy UDF, max samples  must be capped: max_samples=%s, cap=%s
INVALID_FUNCTION_ARGUMENT_INCORRECT_WEIGHT=In differential_entropy UDF, weight must be 1.0: %s
INVALID_FUNCTION_ARGUMENT_INCONSISTENT_MAXSAMPLE=In differential_entropy UDF, inconsistent maxSamples: %s, %s
INVALID_FUNCTION_ARGUMENT_THIRD_ARGUMENT_NEGATIVE=third argument of max_by/min_by must be a positive integer
INVALID_FUNCTION_ARGUMENT_THIRD_ARGUMENT_GREATER_THAN_MAX_VALUE=third argument of max_by/min_by must be less than or equal to %s; found %s
INVALID_FUNCTION_ARGUMENT_NOISE_SCALE_LESS_THAN_ZERO=Noise scale must be >= 0
INVALID_FUNCTION_ARGUMENT_LOWER_GREATER_THAN_UPPER=Lower must be <= upper
INVALID_FUNCTION_ARGUMENT_LOWER_AND_UPPER_NOT_NULL_OR_NON-NULL=Lower and upper should either both null or both non-null
INVALID_FUNCTION_ARGUMENT_INVALID_ARGUMENT=Invalid argument to %s(): NaN
INVALID_FUNCTION_ARGUMENT_COMBINATION_SIZE_NEGATIVE=combination size must not be negative: %s
INVALID_FUNCTION_ARGUMENT_COMBINATION_SIZE_EXCEEDED=combination size must not exceed %s: %s
INVALID_FUNCTION_ARGUMENT_COMBINATION_EXCEEDS_MAX_SIZE=combinations exceed max size
INVALID_FUNCTION_ARGUMENT_NUMBER_OF_COMBINATIONS_TOO_LARGE=Number of combinations too large for array of size %s and combination length %s
INVALID_FUNCTION_ARGUMENT_ONLY_ONE_ARGUMENT=There must be two or more arguments to %s
INVALID_FUNCTION_ARGUMENT_ARRAY_INDEX_STARTS_AT_ONE=SQL array indices start at 1
INVALID_FUNCTION_ARGUMENT_NULL_NOT_SUPPORTED=FIND_FIRST finds NULL as match, which is not supported.
INVALID_FUNCTION_ARGUMENT_UNSUPPORTED_INPUT_TYPE=Input type %s not supported
INVALID_FUNCTION_ARGUMENT_N_NEGATIVE=N must be positive
INVALID_FUNCTION_ARGUMENT_P_LESS_THAN_ZERO=array_normalize only supports non-negative p: %s
INVALID_FUNCTION_ARGUMENT_INVALID_INSTANCE_POSITION=0 is an invalid instance position for array_position.
INVALID_FUNCTION_ARGUMENT_INSTANCE_ARGUMENT_ZERO=array_position cannot take a 0-valued instance argument.
INVALID_FUNCTION_ARGUMENT_LENGTH_LESS_THAN_ZERO=length must be greater than or equal to 0
INVALID_FUNCTION_ARGUMENT_COMPARATOR_CONTRACT_VIOLATION=Lambda comparator violates the comparator contract
INVALID_FUNCTION_ARGUMENT_COMPARATOR_NOT_IN_RESULT_SET=Lambda comparator must return either -1, 0, or 1
INVALID_FUNCTION_ARGUMENT_ELEMENTS_NOT_SUPPORTED_FOR_COMPARISON=Array contains elements not supported for comparison
INVALID_FUNCTION_ARGUMENT_ARRAY_SUBSCRIPT_NEGATIVE=Array subscript is negative
INVALID_FUNCTION_ARGUMENT_ARRAY_SUBSCRIPT_OUT_OF_BOUNDS=Array subscript out of bounds
INVALID_FUNCTION_ARGUMENT_NEGATIVE_SIZE=size must not be negative: %s
INVALID_FUNCTION_ARGUMENT_SIZE_EXCEEDS_ARRAY_CARDINALITY=size must not exceed array cardinality %s: %s
INVALID_FUNCTION_ARGUMENT_BITS_NOT_IN_RANGE=Bits specified in bit_count must be between 2 and 64, got %d
INVALID_FUNCTION_ARGUMENT_NUMBER_NOT_REPRESENTABLE_WITH_BITS=Number must be representable with the bits specified. %d can not be represented with %d bits
INVALID_FUNCTION_ARGUMENT_NEGATIVE_SHIFT=Specified shift must be positive
INVALID_FUNCTION_ARGUMENT_INVALID_COLOR=Invalid color: '%s'
INVALID_FUNCTION_ARGUMENT_RED_NOT_BETWEEN_0_AND_255=red must be between 0 and 255
INVALID_FUNCTION_ARGUMENT_GREEN_NOT_BETWEEN_0_AND_255=green must be between 0 and 255
INVALID_FUNCTION_ARGUMENT_BLUE_NOT_BETWEEN_0_AND_255=blue must be between 0 and 255
INVALID_FUNCTION_ARGUMENT_LOWCOLOR_IS_NOT_RGB=lowColor not a valid RGB color
INVALID_FUNCTION_ARGUMENT_HIGHCOLOR_IS_NOT_RGB=highColor not a valid RGB color
INVALID_FUNCTION_ARGUMENT_COLOR_IS_NOT_RGB=color is not a valid rgb value
INVALID_FUNCTION_ARGUMENT_ONLY_ONE_CONCATENATION_ARGUMENT=There must be two or more concatenation arguments
INVALID_FUNCTION_ARGUMENT_CONCATENATED_STRING_TOO_LARGE=Concatenated string is too large
INVALID_FUNCTION_ARGUMENT_INVALID_DATA_SIZE=Invalid data size: '%s'
INVALID_FUNCTION_ARGUMENT_INVALID_TIMEZONE_OFFSET_INTERVAL=Invalid time zone offset interval: interval contains seconds
INVALID_FUNCTION_ARGUMENT_INVALID_DATE_FIELD='%s' is not a valid DATE field
INVALID_FUNCTION_ARGUMENT_INVALID_TIME_FIELD='%s' is not a valid Time field
INVALID_FUNCTION_ARGUMENT_INVALID_TIMESTAMP_FIELD='%s' is not a valid Timestamp field
INVALID_FUNCTION_ARGUMENT_CONTAINS_Z_IN_TIMESTAMP_FORMAT=format_datetime for TIMESTAMP type, cannot use 'Z' nor 'z' in format, as this type does not contain TZ information
INVALID_FUNCTION_ARGUMENT_CHARACTER_NOT_SUPPORTED=%%%s not supported in date format string
INVALID_FUNCTION_ARGUMENT_IPV4_SUBNET_SIZE_NOT_IN_RANGE=IPv4 subnet size must be in range [0, 32]
INVALID_FUNCTION_ARGUMENT_IPV6_SUBNET_SIZE_NOT_IN_RANGE=IPv6 subnet size must be in range [0, 128]
INVALID_FUNCTION_ARGUMENT_INVALID_IP=Invalid IP address binary: %s
INVALID_FUNCTION_ARGUMENT_ILLEGAL_REPLACEMENT_SEQUENCE=Illegal replacement sequence: %s
INVALID_FUNCTION_ARGUMENT_ILLEGAL_REPLACEMENT_SEQUENCE_UNKNOWN_GROUP=Illegal replacement sequence: unknown group { %s }
INVALID_FUNCTION_ARGUMENT_NEGATIVE_GROUP=Group cannot be negative
INVALID_FUNCTION_ARGUMENT_CANNOT_ACCESS_GROUP=Pattern has %d groups. Cannot access group %d
INVALID_FUNCTION_ARGUMENT_INDEX_OUT_OF_BOUND=Index out of bounds
INVALID_FUNCTION_ARGUMENT_INVALID_JSON_VALUE=Invalid JSON value: %s
INVALID_FUNCTION_ARGUMENT_CANNOT_CONVERT_TO_JSON=Cannot convert '%s' to JSON
INVALID_FUNCTION_ARGUMENT_INVALID_JSON_PATH=Invalid JSON path: '%s'
INVALID_FUNCTION_ARGUMENT_CONCATENATION_ARGUMENT_NOT_MORE_THAN_ONE=There must be two or more concatenation arguments to %s
INVALID_FUNCTION_ARGUMENT_KEY_AND_VALUE_ARRAYS_ARE_OF_DIFFERENT_LENGTH=Key and value arrays must be the same length
INVALID_FUNCTION_ARGUMENT_INDETERMINATE_MAP_KEY=map key cannot be indeterminate: %s
INVALID_FUNCTION_ARGUMENT_MAP_ENTRY_NULL=map entry cannot be null
INVALID_FUNCTION_ARGUMENT_DUPLICATE_MAP_KEYS=Duplicate keys (%s) are not allowed
INVALID_FUNCTION_ARGUMENT_KEY_NOT_PRESENT_IN_MAP=Key not present in map: %s
INVALID_FUNCTION_ARGUMENT_KEY_NOT_PRESENT=Key not present in map
INVALID_FUNCTION_ARGUMENT_SUCCESSPROBABILITY_NOT_INSIDE_INTERVAL=successProbability must be in the interval [0, 1]
INVALID_FUNCTION_ARGUMENT_NUMBEROFTRIALS_LESS_THAN_ZERO=numberOfTrials must be greater than 0
INVALID_FUNCTION_ARGUMENT_P_NOT_INSIDE_INTERVAL=p must be in the interval [0, 1]
INVALID_FUNCTION_ARGUMENT_BOUND_NEGATIVE=bound must be positive
INVALID_FUNCTION_ARGUMENT_UPPER_BOUND_LESS_THAN_LOWER_BOUND=upper bound must be greater than lower bound
INVALID_FUNCTION_ARGUMENT_NEGATIVE_P=p must be 0 > p > 1
INVALID_FUNCTION_ARGUMENT_NEGATIVE_SD=sd must be > 0
INVALID_FUNCTION_ARGUMENT_LESS_THAN_ZERO=a must be > 0
INVALID_FUNCTION_ARGUMENT_LESS_THAN_ZERO_VALUE=b must be > 0
INVALID_FUNCTION_ARGUMENT_VALUE_NOT_INSIDE_INTERVAL=value must be in the interval [0, 1]
INVALID_FUNCTION_ARGUMENT_SCALE_LESS_THAN_ZERO=scale must be greater than 0
INVALID_FUNCTION_ARGUMENT_DF_LESS_THAN_ZERO=df must be greater than 0
INVALID_FUNCTION_ARGUMENT_NEGATIVE_VALUE=value must non-negative
INVALID_FUNCTION_ARGUMENT_NUMERATOR_DF_LESS_THAN_ZERO=numerator df must be greater than 0
INVALID_FUNCTION_ARGUMENT_DENOMINATOR_DF_LESS_THAN_ZERO=denominator df must be greater than 0
INVALID_FUNCTION_ARGUMENT_SHAPE_LESS_THAN_ZERO=shape must be greater than 0
INVALID_FUNCTION_ARGUMENT_VALUE_LESS_THAN_ZERO=value must be greater than, or equal to, 0
INVALID_FUNCTION_ARGUMENT_LAMBDA_LESS_THAN_ZERO=lambda must be greater than 0
INVALID_FUNCTION_ARGUMENT_NOT_A_VALID_NUMBER_FORMAT=Not a valid base-%d number: %s
INVALID_FUNCTION_ARGUMENT_RADIX_NOT_IN_RANGE=Radix must be between %d and %d
INVALID_FUNCTION_ARGUMENT_BUCKETCOUNT_LESS_THAN_ZERO=bucketCount must be greater than 0
INVALID_FUNCTION_ARGUMENT_OPERAND_IS_NAN=operand must not be NaN
INVALID_FUNCTION_ARGUMENT_INFINITE_FIRST_BOUND=first bound must be finite
INVALID_FUNCTION_ARGUMENT_INFINITE_SECOND_BOUND=second bound must be finite
INVALID_FUNCTION_ARGUMENT_EQUAL_BOUNDS=bounds cannot equal each other
INVALID_FUNCTION_ARGUMENT_EMPTY_ARRAY=Bins cannot be an empty array
INVALID_FUNCTION_ARGUMENT_NOT_SORTED_ASCENDING=Bin values are not sorted in ascending order
INVALID_FUNCTION_ARGUMENT_BIN_VALUE_NOT_FINITE=Bin value must be finite, got
INVALID_FUNCTION_ARGUMENT_QUANTILE_NOT_IN_RANGE=Quantile should be within bounds [0, 1], was: %d
INVALID_FUNCTION_ARGUMENT_NEGATIVE_SCALE_FACTOR=Scale factor should be positive.
INVALID_FUNCTION_ARGUMENT_ACCURACY_NOT_IN_RANGE=Percentile accuracy must be exclusively between 0 and 1, was %s
INVALID_FUNCTION_ARGUMENT_PERCENTILE_WEIGHT_LESS_THAN_ZERO=Percentile weight must be > 0, was %s
INVALID_FUNCTION_ARGUMENT_NOT_NULL_VALUES=expect null values
INVALID_FUNCTION_ARGUMENT_COUNT_ARGUMENT_OF_REPEAT_FUNCTION_GREATER_THAN_10000=count argument of repeat function must be less than or equal to 10000
INVALID_FUNCTION_ARGUMENT_COUNT_ARGUMENT_OF_REPEAT_FUNCTION_LESS_THAN_0=count argument of repeat function must be greater than or equal to 0
INVALID_FUNCTION_ARGUMENT_REPEAT_FUNCTION_EXCEEDS_MAX_SIZE_IN_BYTES=result of repeat function must not take more than 1000000 bytes
INVALID_FUNCTION_ARGUMENT_SIZE_DOES_NOT_MATCH=the size of fromType and toType must match
INVALID_FUNCTION_ARGUMENT_STEP_NOT_A_DAY_INTERVAL=sequence step must be a day interval if start and end values are dates
INVALID_FUNCTION_ARGUMENT_STEP_EQUAL_TO_ZERO=step must not be zero
INVALID_FUNCTION_ARGUMENT_STOP_VALUE_NOT_EQUAL_TO_START_VALUE=sequence stop value should be greater than or equal to start value if step is greater than zero otherwise stop should be less than or equal to start
INVALID_FUNCTION_ARGUMENT_SEQUENCE_FUNCTION_HAS_MORE_THAN_10000_ENTRIES=result of sequence function must not have more than 10000 entries
INVALID_FUNCTION_ARGUMENT_ENTRYDELIMITER_EMPTY=entryDelimiter is empty
INVALID_FUNCTION_ARGUMENT_KEYVALUEDELIMITER_EMPTY=keyValueDelimiter is empty
INVALID_FUNCTION_ARGUMENT_SAME_ENTRYDELIMITER_AND_KEYVALUEDEMLIMITER=entryDelimiter and keyValueDelimiter must not be the same
INVALID_FUNCTION_ARGUMENT_BAD_INPUT=Key-value delimiter must appear exactly once in each entry. Bad input: %s
INVALID_FUNCTION_ARGUMENT_DUPLICATE_KEYS=Duplicate keys (%s) are not allowed. Specifying a lambda to resolve conflicts can avoid this error
INVALID_FUNCTION_ARGUMENT_INVALID_CODE_POINT=Not a valid Unicode code point: %d
INVALID_FUNCTION_ARGUMENT_INPUT_STRING_NOT_SINGLE_CHARACTER=Input string must be a single character string
INVALID_FUNCTION_ARGUMENT_INPUTES_TO_REPLACE_FUNCTION_ARE_TOO_LARGE=inputs to \replace\" function are too large: when \"search\" parameter is empty, length of \"string\" times length of \"replace\" must not exceed "
INVALID_FUNCTION_ARGUMENT_INSTANCE_NEGATIVE='instance' must be a positive number.
INVALID_FUNCTION_ARGUMENT_NEGATIVE_LIMIT=Limit must be positive
INVALID_FUNCTION_ARGUMENT_LIMIT_TOO_LARGE=Limit is too large
INVALID_FUNCTION_ARGUMENT_INDEX_LESS_THAN_ZERO=Index must be greater than zero
INVALID_FUNCTION_ARGUMENT_INVALID_ENCODING=Invalid UTF-8 encoding
INVALID_FUNCTION_ARGUMENT_INVALID_ENCODING_IN_CHARACTER=Invalid UTF-8 encoding in characters: %s
INVALID_FUNCTION_ARGUMENT_PADDING_STRING_EMPTY=Padding string must not be empty
INVALID_FUNCTION_ARGUMENT_IMPUT_TOO_LARGE=The combined inputs for Levenshtein distance are too large
INVALID_FUNCTION_ARGUMENT_INPUT_STRING_TO_HAMMING_DISTANCE_NOT_SAME=The input strings to hamming_distance function must have the same length
INVALID_FUNCTION_ARGUMENT_NORMALIZATION_FORM_NOT_IN_LIST=Normalization form must be one of [NFD, NFC, NFKD, NFKC]
INVALID_FUNCTION_ARGUMENT_REPLACEMENT_CHARACTER_NOT_EMPTY_OR_SINGLE_CHARACTER=Replacement character string must empty or a single character
INVALID_FUNCTION_ARGUMENT_INVALID_REPLACEMENT_CHARACTER=Invalid replacement character
INVALID_FUNCTION_ARGUMENT_LOWER_QUANTILE_NOT_ZERO_OR_ONE=Lower quantile bound should be in [0,1].
INVALID_FUNCTION_ARGUMENT_UPPER_QUANTILE_NOT_ZERO_OR_ONE=Upper quantile bound should be in [0,1].
INVALID_FUNCTION_ARGUMENT_INVALID_INPUT_LENGTH=invalid input length %d
INVALID_FUNCTION_ARGUMENT_NOT_EXPECTED_8BYTE_INPUT=expected 8-byte input, but got instead: %d
INVALID_FUNCTION_ARGUMENT_NOT_EXPECTED_4BYTE_INPUT=expected 4-byte input, but got instead: %d
INVALID_FUNCTION_ARGUMENT_FLOATING_POINT_VALUE_NOT_4_BYTES_LONG=Input floating-point value must be exactly 4 bytes long
INVALID_FUNCTION_ARGUMENT_FLOATING_POINT_VALUE_NOT_8_BYTES_LONG=Input floating-point value must be exactly 8 bytes long
INVALID_FUNCTION_ARGUMENT_INVALID_HEX_CHARACTER=invalid hex character: %s
INVALID_FUNCTION_ARGUMENT_PADDING_BYTES_EMPTY=Padding bytes must not be empty
INVALID_FUNCTION_ARGUMENT_NUMBER_SUCCESS_NEGATIVE=number of successes must not be negative
INVALID_FUNCTION_ARGUMENT_NEGATIVE_TRAILS=number of trials must be positive
INVALID_FUNCTION_ARGUMENT_NUMBER_OF_SUCCESS_LESS_THAN_TRIALS=number of successes must not be larger than number of trials
INVALID_FUNCTION_ARGUMENT_Z-SCORE_NEGATIVE=z-score must not be negative
INVALID_FUNCTION_ARGUMENT_UNKNOWN_STEMMER_LANGUAGE=Unknown stemmer language: %s
INVALID_FUNCTION_ARGUMENT_OFFSET_LESS_THAN_ZERO=Offset must be at least 0
INVALID_FUNCTION_ARGUMENT_OFFSET_LESS_THAN_1=Offset must be at least 1
INVALID_FUNCTION_ARGUMENT_BUCKETS_LESS_THAN_ZERO=Buckets must be greater than 0
INVALID_FUNCTION_ARGUMENT_CANNOT_UNNEST_TYPE=Cannot unnest type: %s
INVALID_FUNCTION_ARGUMENT_ROW_INDEX_OUT_OF_BOUNDS=ROW index out of bounds: %d
INVALID_FUNCTION_ARGUMENT_CANNOT_ADD_HOUR_MINUTES_SECONDS=Cannot add hour, minutes or seconds to a date
INVALID_FUNCTION_ARGUMENT_CANNOT_SUBTRACT_HOUR_MINUTES_SECONDS=Cannot subtract hour, minutes or seconds from a date
INVALID_FUNCTION_ARGUMENT_ESCAPE_STRING_NOT_A_SINGLE_CHARACTER=Escape string must be a single character
INVALID_FUNCTION_ARGUMENT_ESCAPE_CHARACTER_NOT_FOLLOWED_BY_CHARACTER=Escape character must be followed by '%%', '_' or the escape character itself
INVALID_FUNCTION_ARGUMENT_NO_VALUE_IN_ENUM_TYPE=No value '%d' in enum type %s
INVALID_FUNCTION_ARGUMENT_UNSUPPORTED_TYPE=Unsupported type: %s
INVALID_FUNCTION_ARGUMENT_UNKNOWN_PARAMETER=Unknown parameter %s
INVALID_FUNCTION_ARGUMENT_UNKNOWN_KERNEL_TYPE=Unknown kernel type %s
INVALID_FUNCTION_ARGUMENT_CANNOT_SPECIFY_TAGS=Primary read preference can not specify tag sets
INVALID_FUNCTION_ARGUMENT_COUNT_LESS_THAN_ZERO=count must be greater than or equal to zero
DIVISION_BY_ZERO=Division by zero
INVALID_CAST_ARGUMENT_CANNOT_CAST_VALUE_TO_IP=Cannot cast value to IPADDRESS: %s
INVALID_CAST_ARGUMENT_INVALID_BIGINT_TITLE_ENCODING=Invalid bigint tile encoding: %s
INVALID_CAST_ARGUMENT_INVALID_JSON_STRING=Invalid JSON string for KDB tree
INVALID_CAST_ARGUMENT_CANNOT_CAST_TO_JSON=Cannot cast %s to JSON
INVALID_CAST_ARGUMENT_CANNOT_CAST_JSON_TO_VARCHAR=Cannot cast input json to VARCHAR
INVALID_CAST_ARGUMENT_CANNOT_CAST=Cannot cast '%s' to %s
INVALID_CAST_ARGUMENT_CANNOT_CAST_JSON_TO_BIGINT=Cannot cast input json to BIGINT
INVALID_CAST_ARGUMENT_CANNOT_CAST_JSON_TO_INTEGER=Cannot cast input json to INTEGER
INVALID_CAST_ARGUMENT_CANNOT_CAST_JSON_TO_SMALLINT=Cannot cast input json to SMALLINT
INVALID_CAST_ARGUMENT_CANNOT_CAST_JSON_TO_TINYINT=Cannot cast input json to TINYINT
INVALID_CAST_ARGUMENT_CANNOT_CAST_JSON_TO_DOUBLE=Cannot cast input json to DOUBLE
INVALID_CAST_ARGUMENT_CANNOT_CAST_JSON_TO_REAL=Cannot cast input json to REAL
INVALID_CAST_ARGUMENT_CANNOT_CAST_JSON_TO_BOOLEAN=Cannot cast input json to BOOLEAN
INVALID_CAST_ARGUMENT_CANNOT_CAST_JSON_TO_ARRATTYPE=Cannot cast JSON to %s
INVALID_CAST_ARGUMENT_CANNOT-CAST_TO_ARRAYTYPE=Cannot cast to %s. %s%n%s
INVALID_CAST_ARGUMENT_CANNOT_CAST_TYPE=Cannot cast to %s.%n%s
INVALID_CAST_ARGUMENT_MAP_KEY_NULL=map key is null
INVALID_CAST_ARGUMENT_DUPLICATE_KEY=duplicate keys
INVALID_CAST_ARGUMENT_VALUE_CANNOT_BE_REPRESENTED_AS_VARCHAR=Value %s cannot be represented as varchar(%s)
INVALID_CAST_ARGUMENT_CANNOT_CAST_TO_DATE=Value cannot be cast to date: %s
INVALID_CAST_ARGUMENT_CANNOT_CAST_TO_BIGINT=Cannot cast '%s' to BIGINT
INVALID_CAST_ARGUMENT_CANNOT_CAST_BIGINT_TO_DECIMAL=Cannot cast BIGINT '%s' to DECIMAL(%s, %s)
INVALID_CAST_ARGUMENT_CANNOT_CAST_TO_INTEGER=Cannot cast '%s' to INTEGER
INVALID_CAST_ARGUMENT_CANNOT_CAST_INTEGER_TO_DECIMAL=Cannot cast INTEGER '%s' to DECIMAL(%s, %s)
INVALID_CAST_ARGUMENT_CANNOT_CAST_TO_SMALLINT=Cannot cast '%s' to SMALLINT
INVALID_CAST_ARGUMENT_CANNOT_CAST_SMALLINT_TO_DECIMAL=Cannot cast SMALLINT '%s' to DECIMAL(%s, %s)
INVALID_CAST_ARGUMENT_CANNOT_CAST_TO_TINYINT=Cannot cast '%s' to TINYINT
INVALID_CAST_ARGUMENT_CANNOT_CAST_TINYINT_TO_DECIMAL=Cannot cast TINYINT '%s' to DECIMAL(%s, %s)
INVALID_CAST_ARGUMENT_CANNOT_CAST_DOUBLE_TO_DECIMAL=Cannot cast DOUBLE '%s' to DECIMAL(%s, %s)
INVALID_CAST_ARGUMENT_CANNOT_CAST_REAL_TO_DECIMAL=Cannot cast REAL '%s' to DECIMAL(%s, %s)
INVALID_CAST_ARGUMENT_CANNOT_CAT_VARCHAR_TO_DECIMAL=Cannot cast VARCHAR '%s' to DECIMAL(%s, %s). Value is not a number.
INVALID_CAST_ARGUMENT_CANNOT_CAST_VARCHAR_TO_DECIMAL=Cannot cast VARCHAR '%s' to DECIMAL(%s, %s). Value too large.
INVALID_CAST_ARGUMENT_CANNOT_CAST_DECIMAL_TO_TYPE=Cannot cast '%f' to %s
INVALID_CAST_ARGUMENT_CANNOT_CAST_INPUT_JSON_TO_DECIMAL=Cannot cast input json to DECIMAL(%s,%s)
INVALID_CAST_ARGUMENT_CANNOT_CAST_JSON_TO_DECIMAL=Cannot cast '%s' to DECIMAL(%s,%s)
INVALID_CAST_ARGUMENT_CANNOT_CAST_DECIMAL_TO_DECIMAL=Cannot cast DECIMAL '%s' to DECIMAL(%d, %d)
INVALID_CAST_ARGUMENT_CANNOT_CAST_TYPE_TO_TYPE=Cannot cast %s to %s
INVALID_CAST_ARGUMENT_CANNOT_CAST_VALUE_TO_BIGINT=Unable to cast %s to bigint
INVALID_CAST_ARGUMENT_VARCHAR_VALUE_NOT_IN_ENUM=No value '%s' in enum '%s'
INVALID_CAST_ARGUMENT_LONG_VALUE_NOT_IN_ENUM=No value '%d' in enum '%s'
INVALID_CAST_ARGUMENT_CANNOT_CAST_TO_IPADDRESS=Cannot cast value to IPADDRESS: %s
INVALID_CAST_ARGUMENT_INVALID_IP_ADDRESS_LENGTH=Invalid IP address binary length: %d
INVALID_CAST_ARGUMENT_CANNOT_CAST_TO_IPPREFIX=Cannot cast value to IPPREFIX: %s
INVALID_CAST_ARGUMENT_INVALID_IP_ADDRESS=Invalid IP address binary: %s
INVALID_CAST_ARGUMENT_CANNOT_CAST_TO_TIME=Value cannot be cast to time: %s
INVALID_CAST_ARGUMENT_CANNOT_CAST_TO_TIMESTAMP=Value cannot be cast to timestamp: %s
INVALID_CAST_ARGUMENT_CANNOT_CAST_TO_TIMESTAMP_WITH_TIMEZONE=Value cannot be cast to timestamp with time zone: %s
INVALID_CAST_ARGUMENT_INVALID_UUID=Invalid UUID string length: %d
INVALID_CAST_ARGUMENT_CANNOT_CAST_VALUE_TO_UUID=Cannot cast value to UUID: %s
INVALID_CAST_ARGUMENT_INVALID_UUID_BINARY_LENGTH=Invalid UUID binary length: %d
INVALID_CAST_ARGUMENT_CANNOT_CAST_TO_BOOLEAN=Cannot cast '%s' to BOOLEAN
INVALID_CAST_ARGUMENT_CANNOT_CAST_TO_DOUBLE=Cannot cast '%s' to DOUBLE
INVALID_CAST_ARGUMENT_CANNOT_CAST_TO_REAL=Cannot cast '%s' to REAL
INVALID_CAST_ARGUMENT_CANNOT_CAST_TO_INT=Cannot cast '%s' to INT
INVALID_VIEW_ALREADY_EXISTS=View already exists as data table
INVALID_VIEW_INVALID_MATERIALIZED_VIEW=Invalid materialized view JSON
INVALID_VIEW_JSON=Invalid view JSON: %s
ALREADY_EXISTS_VIEW_ALREADY_EXISTS=View already exists
ALREADY_EXISTS_SCHEMA_ALREADY_EXITS=Schema [%s] already exists
ALREADY_EXISTS_TYPE-ALREADY_EXISTS=Type %s already exists
ALREADY_EXISTS_FUNCTION_ALREADY_EXISTS=Function already exists: %s
ALREADY_EXISTS_PARTITION_ALREADY_EXISTS=Partition already exists
ALREADY_EXISTS_ROLE_NAME_AS_RESERVED_ROLES=Role name cannot be one of the reserved roles: %s
ALREADY_EXISTS_ROLE_ALREADY_EXISTS=Role already exists: '%s'
ALREADY_EXISTS_PARTITION_ALREADY_EXISTS_FOR_TABLE=Partition already exists for table '%s.%s': %s
ALREADY_EXISTS_COLUMN_ALREADY_EXISTS=Column already exists: %s
ALREADY_EXISTS_ONE_OR_PARTITION_ALREADY_EXISTS=One or more partitions already exist for table '%s.%s'
ALREADY_EXISTS_SESSION_FUNCTION_ALREADY_DEFINED=Session function %s has already been defined
ALREADY_EXISTS_MATERIALIZED_ALREADY_EXISTS=Materialized view already exists
ALREADY_EXISTS_TABLE_ALREADY_EXISTS=Table already exists
ALREADY_EXISTS_VIEW_WITH_THE_NAME_ALREADY_EXISTS=View already exists: %s
ALREADY_EXISTS_TABLE_WITH_THE_NAME_ALREADY_EXISTS=Table [%s] already exists
NOT_SUPPORTED_ACCUMULO_DOES_NOT_SUPPORT_TABLE_TO_DIFFERENT_NAMESAPCE=Accumulo does not support renaming tables to different namespaces (schemas)
NOT_SUPPORTED_UNABLE_TO_ROLLBACK_INSERT_FOR_TABLE=Unable to rollback insert for table %s.%s. Some rows may have been written. Please run your insert again.
NOT_SUPPORTED_NO_INDEXED_COLUMNS_IN_TABLE_METADATA=No indexed columns in table metadata. Refusing to index a table with no indexed columns
NOT_SUPPORTED_UNSUPPORTED_TYPE=Unsupported type %s
NOT_SUPPORTED_UNSUPPORTED_PRESTO_TYPE=Unsupported PrestoType %s
NOT_SUPPORTED_NO_LEXICODER_FOR_TYPE=No lexicoder for type %s
NOT_SUPPORTED_ARRAYS_NOT_SUPPORTED_FOR_STRINGROWSERIALIZER=arrays are not (yet?) supported for StringRowSerializer
NOT_SUPPORTED_MAPS_NOT_SUPPORTED_FOR_STRINGROWSERIALIZER=maps are not (yet?) supported for StringRowSerializer
NOT_SUPPORTED_STRINGLEXICODER_DOES_NOT_SUPPORT_ENCODING_TYPE=StringLexicoder does not support encoding type %s, object class is %s
NOT_SUPPORTED_STRINGLEXICODER_DOES_NOT_SUPPORT_DECODING_TYPE=StringLexicoder does not support decoding type
NOT_SUPPORTED_EXPLAIN_ANALYZE_NOT_SUPPORT_STATEMENT_TYPE=EXPLAIN ANALYZE does not support statement type: %s
NOT_SUPPORTED_TABLEHANDLE_GREATER_THAN_ONE=Multiple tables matched: %s
NOT_SUPPORTED_UNSUPPORTED_COLUMN_TYPE=Unsupported column type: %s
NOT_SUPPORTED_VIEWS_NOT_ENABLED=Views are not enabled. You can enable views by setting 'bigquery.views-enabled' to true. Notice additional cost may occur.
NOT_SUPPORTED_TABLE_TYPE_NOT_SUPPORTED=Table type '%s' of table '%s.%s' is not supported
NOT_SUPPORTED_DISTRIBUTED_READS_FOR_BLACK_HOLE_CONNECTOR=Black hole connector does not supported distributed reads
NOT_SUPPORTED_DROPPING_MATERIALIZED_VIEWS=Dropping materialized views not yet supported
NOT_SUPPORTED_RENAMING_TABLES_FOR_CASSANDRA=Renaming tables not yet supported for Cassandra
NOT_SUPPORTED_INSERTING_INTO_MATERIALIZED_VIEWS=Inserting into materialized views not yet supported
NOT_SUPPORTED_CLUSTERING_KEY_TYPE=Unsupported clustering key type: %s
NOT_SUPPORTED_MORE_THAN_ONE_KEYSPACE_FOUND=More than one keyspace has been found for the case insensitive schema name: %s -> (%s, %s)
NOT_SUPPORTED_MORE_THAN-ONE_TABLE_FOUND=More than one table has been found for the case insensitive table name: %s -> (%s)
NOT_SUPPORTED_MORE_THAN_ONE_COLUMN_FOUND=More than one column has been found for the case insensitive column name: %s -> (%s, %s)
NOT_SUPPORTED_CASSANDRA_VERSIONS_PRIOR_TO_2.1.5=Cassandra versions prior to 2.1.5 are not supported
NOT_SUPPORTED_CANNOT_CREATE_MANAGED_DELTA_TABLE=Cannot create managed Delta table
NOT_SUPPORTED_NOT_THE_EXPECTED_TABLE_NAME_FORMAT=Invalid Delta table name: %s, Expected table name form 'tableName[@v<snapshotId>][@t<snapshotAsOfTimestamp>]'. The table can have either a particular snapshot identifier or a timestamp of the snapshot. If timestamp is given the latest snapshot of the table that was generated at or before the given timestamp is read
NOT_SUPPORTED_TIMESTAMP_FORMAT_NOT_VALID=Invalid Delta table name: %s, given snapshot timestamp (%s) format is not valid.  Expected timestamp format 'YYYY-MM-DD HH:mm:ss'
NOT_SUPPORTED_INVALID_DELTA_TABLE_NAME=Invalid Delta table name:   , Table suffix contains both snapshot id and timestamp of snapshot to read. Only one of them is supported.
NOT_SUPPORTED_UNSUPPORTED-REPRESENTATION_FOR_FIELDS=Unsupported representation for field '%s' of type TIMESTAMP: %s [%s]
NOT_SUPPORTED_ALTER_FUNCTION_IN_JSONFILEBASEDFUNCTIONNAMESPACEMANAGER=Alter Function is not supported in JsonFileBasedFunctionNamespaceManager
NOT_SUPPORTED_DROP_FUNCTION_JSONFILEBASEDFUNCTIONNAMESPACEMANAGER=Drop Function is not supported in JsonFileBasedFunctionNamespaceManager
NOT_SUPPORTED_ALTER_FUNCTION_IN_INMEMORYFUNCTIONNAMESPACEMANAGER=Alter Function is not supported in InMemoryFunctionNamespaceManager
NOT_SUPPORTED_DROP_FUNCTION_IN_INMEMORYFUNCTIONNAMESPACEMANAGER=Drop Function is not supported in InMemoryFunctionNamespaceManager
NOT_SUPPORTED_COERCER=Unsupported Coercer from %s to %s
NOT_SUPPORTED_COERCER_FROM_VARCHAR=Could not create Coercer from varchar to %s
NOT_SUPPORTED_MATERIALIZED_VIEW_DOES_NOT_HAVE_ATLEAST_ONE_COLUMN_DEFINED=Materialized view %s must have at least one column directly defined by a base table column.
NOT_SUPPORTED_UNPARTITIONED_MATERIALIZED_VIEW=Unpartitioned materialized view is not supported.
NOT_SUPPORTED_MATERIALIZED_VIEW_DOES_NOT_HAVE_PARTITION_COLUMN_THAT_EXISTS_IN_TABLE=Materialized view %s must have at least one partition column that exists in %s as well
NOT_SUPPORTED_OUTER_JOIN_IN_MATERIALIZED_VIEW_DOES_NOT_HAVE_COMMON_PARTITION=Outer join conditions in Materialized view %s must have at least one common partition equality constraint
NOT_SUPPORTED_HIVE_TYPE_FOUND_IN_PARTITION_KEY=Unsupported Hive type %s found in partition keys of table %s.%s
NOT_SUPPORTED_UNEXPECTED_TABLE_IN_HIVE_METASTORE=Unexpected table present in Hive metastore: %s
NOT_SUPPORTED_PARTITIONING_WHEN_AVRO_SCHEMA_URL_SET=Bucketing/Partitioning columns not supported when Avro schema url is set
NOT_SUPPORTED_CREATE_NON-MANAGED_HIVE_TABLE=Cannot create non-managed Hive table
NOT_SUPPORTED_COLUMN_HAS_NON-VARCHAR_MAP_KEY=Column %s has a non-varchar map key, which is not supported by Avro
NOT_SUPPORTED_COLUMN_IS_TINYINT=Column %s is tinyint, which is not supported by Avro. Use integer instead.
NOT_SUPPORTED_COLUMN_IS_SMALLINT=Column %s is smallint, which is not supported by Avro. Use integer instead.
NOT_SUPPORTED_ALTER_TABLE_WHEN_AVRO_SCHEMA_SET=ALTER TABLE not supported when Avro schema url is set
NOT_SUPPORTED_EXTERNAL_TABLE_CANNOT_BE_CREATED=External tables cannot be created using CREATE TABLE AS
NOT_SUPPORTED_CREATE_TABLE_AS_WHEN_AVRO-SCHEMA_SET=CREATE TABLE AS not supported when Avro schema url is set
NOT_SUPPORTED_INSERTING_INTO_HIVE_TABLE_WITH_THE_COLUMN_TYPE=Inserting into Hive table %s with column type %s not supported
NOT_SUPPORTED_OVERWRITING_EXISTING_PARTITION_IN_TRANSCATIONAL_TABLE=overwriting existing partition in transactional tables does not support DIRECT_TO_TARGET_EXISTING_DIRECTORY write mode
NOT_SUPPORTED_PARTITION_NOT_DELETED_ENTIRELY=This connector only supports delete where one or more partitions are deleted entirely
NOT_SUPPORTED_WRITING_TO_BUCKET_SORTED_HIVE_TABLE=Writing to bucketed sorted Hive tables is disabled
NOT_SUPPORTED_HIVE_STORAGE_FORMAT_NOT_DWRF=Only DWRF file format supports encryption at this time
NOT_SUPPORTED_UNSUPPORTED_COLUMNS=Hive CSV storage format only supports VARCHAR (unbounded). Unsupported columns:
NOT_SUPPORTED_UNSUPPORTED_COLUMN_TYPE_FOR_PREFILLED_COLUMN=Unsupported column type %s for prefilled column: %s
NOT_SUPPORTED_UNSUPPORTED_TYPE_FOR_PARTITION=Unsupported type [%s] for partition: %s
NOT_SUPPORTED_UNSUPPORTED_HIVE_TYPE_IN_PARTITION_KEYS=Unsupported Hive type %s found in partition keys of table %s.%s
NOT_SUPPORTED_UNSUPPORTED_HIDDEN_COLUMN=unsupported hidden column: %s
NOT_SUPPORTED_UNSUPPORTED_COLUMN_TYPE_FOR_PARTITION_COLUMN=Unsupported column type %s for partition column: %s
NOT_SUPPORTED_CANNOT_WRITE_TO_NON-MANAGED_HIVE_TABLE=Cannot write to non-managed Hive table
NOT_SUPPORTED_INSERT_INTO_BUCKET_TABLE_WITH_SKEW=Inserting into bucketed tables with skew is not supported. %s
NOT_SUPPORTED_TABLE_FORMAT_SYMLINKTEXTINPUTFORMAT=Bucketed table in SymlinkTextInputFormat is not yet supported
NOT_SUPPORTED_PARTITION_FORMAT_USEFILESPLITSFROMINPUTFORMAT=Presto cannot read bucketed partition in an input format with UseFileSplitsFromInputFormat annotation:
NOT_SUPPORTED_CONTAINS_INELIGIBLE_TABLE_BUCKET=The bucket filter cannot be satisfied. There are restrictions on the bucket filter when all the following is true: +\
  1. a table has a different buckets count as at least one of its partitions that is read in this query; +\
  2. the table has a different but compatible bucket number with another table in the query; +\
  3. some buckets of the table is filtered out from the query, most likely using a filter on \"$bucket\". (table name: %s, table bucket count: %d,  partition bucket count: %d, effective reading bucket count: %d)
NOT_SUPPORTED_SCHEMA_EVOLUTION_FOR_PAGEFILE_FORMAT=schema evolution is not supported for PageFile format
NOT_SUPPORTED_PARQUET_READER_FILTER_PUSHDOWN=Parquet reader does not support filter pushdown yet
NOT_SUPPORTED_COMPRESSION_EXTENSION_FOR_S3_SELECT=Compression extension not supported for S3 Select: %s
NOT_SUPPORTED_UNSUPPORTED_HIVE_TYPE=Unsupported Hive type: %s
NOT_SUPPORTED_UNSUPPORTED_HIVE_VARCHAR_TYPE=Unsupported Hive type: %s. Supported VARCHAR types: VARCHAR(<=%d), VARCHAR.
NOT_SUPPORTED_UNSUPPORTED_HIVE_CHAR_TYPE=Unsupported Hive type: %s. Supported CHAR types: CHAR(<=%d).
NOT_SUPPORTED_UNSUPPORTED_HIVE_ANONYMOUS_ROW_TYPE=Anonymous row type is not supported in Hive. Please give each field a name: %s
NOT_SUPPORTED_NO_DEFAULT_HIVE_TYPE_PROVIDED=No default Hive type provided for unsupported Hive type: %s
NOT_SUPPORTED_CANNOT_DROP_PARTITION_COLUMN=Cannot drop partition columns
NOT_SUPPORTED_CANNOT_DROP_THE_ONLY_NON-PATITION_COLUMN=Cannot drop the only non-partition column in a table
NOT_SUPPORTED_TYPE=unsupported type:
NOT_SUPPORTED_PARTITION_KEY_TYPE=Unsupported partition key type: %s
NOT_SUPPORTED_CANNOT_DELETE_FOR_NON-MANAGED_HIVE_TABLE=Cannot delete from non-managed Hive table
NOT_SUPPORTED_DROPPING_PARTITION_ADDED_IN_SAME_TRANSACTION=dropping a partition added in the same transaction is not supported: %s %s %s
NOT_SUPPORTED_CANNOT_INSERT_INTO_TABLE_WITH_PARTITION_MODIFIED_IN_SAME_TRANSACTION=Can not insert into a table with a partition that has been modified in the same transaction when Presto is configured to skip temporary directories.
NOT_SUPPORTED_UNSUPPORTED_COMBINATION_OF_OPERATIONS=Unsupported combination of operations in a single transaction
NOT_SUPPORTED_CANNOT_CHANGE_SCHEMA_TO_TABLE_WITH_PARTITION_MODIFIED_IN_SAME_TRANSACTION=Cannot make schema changes to a table/view with modified partitions in the same transaction
NOT_SUPPORTED_TABLE_TYPE=Table type not supported: %s
NOT_SUPPORTED_RENAME_TABLE=Rename not supported for Iceberg tables
NOT_SUPPORTED_RENAME_PARTITION=Renaming partition columns is not supported
NOT_SUPPORTED_ADD_PARTITION=Partitions can not be added to %s
NOT_SUPPORTED_GLUE_METASTORE_COLUMN_LEVEL_STATISTICS_DISABLED=Glue metastore column level statistics are disabled
NOT_SUPPORTED_TABLE_RENAME_BY_GLUE_SERVICE=Table rename is not yet supported by Glue service
NOT_SUPPORTED_RENAME_PARTITION_COLUMN=Renaming partition columns is not supported
NOT_SUPPORTED_CREATEROLE_BY_GLUE=createRole is not supported by Glue
NOT_SUPPORTED_DROPROLE_BY_GLUE=dropRole is not supported by Glue
NOT_SUPPORTED_GRANTROLE_BY_GLUE=grantRoles is not supported by Glue
NOT_SUPPORTED_REVOKEROLES_BY_GLUE=revokeRoles is not supported by Glue
NOT_SUPPORTED_GRANTTABLEPRIVILEGES_BY_GLUE=grantTablePrivileges is not supported by Glue
NOT_SUPPORTED_REVOKETABLEPRVILIGES_BY_GLUE=revokeTablePrivileges is not supported by Glue
NOT_SUPPORTED_LISTTABLEPREVILIGES_BY_GLUE=listTablePrivileges is not supported by Glue
NOT_SUPPORTED_SETPARTITIONLEASE_BY_GLUE=setPartitionLeases is not supported by Glue
NOT_SUPPORTED_RENAMING_SCHEMA_BY_HIVE_METASTORE=Hive metastore does not support renaming schemas
NOT_SUPPORTED_COLUMN_LEVEL_STATISTICS_BY_GLUE_METASTORE=Glue metastore does not support column level statistics
NOT_SUPPORTED_GRANTING_PRIVILEGE_WHEN_GRANTEE_ALREADY_HAS_REQUESTED_PRIVILEGE=Granting %s WITH GRANT OPTION is not supported while %s possesses %s
NOT_SUPPORTED_INVALID_PARTITION_TYPE=Invalid partition type %s
NOT_SUPPORTED_CREATE_PAGE_SOURCE=Could not create page source for table type
NOT_SUPPORTED_SNAPSHOT_ID_FOR_HISTORY_TABLE=Snapshot ID not supported for history table: %s
NOT_SUPPORTED_SNAPSHOT_ID_FOR_SNAPSHOT_TABLE=Snapshot ID not supported for snapshots table: %s
NOT_SUPPORTED_ADD_COLUMN_WITH_NON_NULL=This connector does not support add column with non null
NOT_SUPPORTED_DELETE_DATA_SPECIFIED_SNAPSHOT=This connector do not allow delete data at specified snapshot
NOT_SUPPORTED_FILE_FORMAT_FOR_ICEBERG=File format not supported for Iceberg: %s
NOT_SUPPORTED_DATABASE_LOCATION_NOT_SET=Database %s location is not set
NOT_SUPPORTED_TABLE_CONTAINS_ICEBERG_PATH=Table %s contains Iceberg path override properties and cannot be dropped from Presto
NOT_SUPPORTED_RENAME_NAMESPACE_ICEBERG=Iceberg %s catalog does not support rename namespace
NOT_SUPPORTED_NAMESPACE_OPERATION_FOR_CATALOG_TYPE=Iceberg catalog of type %s does not support namespace operations
NOT_SUPPORTED_INVALID_ICEBERG_TABLE_NAME=Invalid Iceberg table name: %s
NOT_SUPPORTED_INVALID_ICEBERG_TABLE_NAME_WITH_UNKNOWN_TYPE=Invalid Iceberg table name (unknown type '%s'): %s
NOT_SUPPORTED_INVALID_ICEBERG_TABLE_NAME_CANNOT_SPECIFY_TWO_VERSIONS=Invalid Iceberg table name (cannot specify two @ versions): %s
NOT_SUPPORTED_INVALID_ICEBERG_TABLE_NAME_CANNOT_USE_VERSION_WITH_TABLE_TYPE=Invalid Iceberg table name (cannot use @ version with table type '%s'): %s
NOT_SUPPORTED_WRITING_TABLE_WITH_CUSTOM_LOCATION_PROVIDER=Table %s specifies %s as a location provider. Writing to Iceberg tables with custom location provider is not supported.
NOT_SUPPORTED_TABLE_MODE=merge-on-read table mode not supported yet
NOT_SUPPORTED_ICEBERG_TYPE=Type not supported for Iceberg: %s
NOT_SUPPORTED_FILED_DOES_NOT_HAVE_NAME=Row type field does not have a name: %s
NOT_SUPPORTED_TARGET_QUERY_NOT_RUNNING=Target query is not running: %s
NOT_SUPPORTED_STATEMENT_TYPE=Unsupported statement type: %s
NOT_SUPPORTED_CALLING_PROCEDURE_WITHIN_TRANSACTION=Procedures cannot be called within a transaction (use autocommit mode)
NOT_SUPPORTED_INVOKE_DYNAMICALLY_REGISTERED_FUNCTION=Invoking a dynamically registered function in SQL function body is not supported
NOT_SUPPORTED_CASCADE_FOR_DROP_SCHEMA=CASCADE is not yet supported for DROP SCHEMA
NOT_SUPPORTED_INVALID_STATEMENT_TYPE=Invalid statement type for prepared statement:
NOT_SUPPORTED_STATEMENT_TOO_LARGE=statement is too large (stack overflow during analysis)
NOT_SUPPORTED_NESTED_TRANSACTION=Nested transactions not supported
NOT_SUPPORTED_MULTIPLE_LAYOUT_FOR_TABLE=Connector returned multiple layouts for table %s
NOT_SUPPORTED_TABLE_HAS_NO_COLUMN=Table has no columns: %s
NOT_SUPPORTED_STATE_TYPE_NOT_ENABLED=State type not enabled for %s: %s
NOT_SUPPORTED_T-DIGEST_WITH_REAL_NUMBERS=Cannot operate on a t-digest with real numbers
NOT_SUPPORTED_CANNOT_CREATE_ON_T-DIGEST=Cannot operate on a t-digest with longs
NOT_SUPPORTED_TOO_MANY_ARGUMENT_FOR_FUNCTION_CALL=Too many arguments for function call %s()
NOT_SUPPORTED_TOO_MANY_ARGUMENT_FOR_ARRAY_CONSTRUCTOR=Too many arguments for array constructor
NOT_SUPPORTED_CONTAINS_DOES_SUPPORT_ARRAYS_WITH_NULL=contains does not support arrays with elements that are null or contain null
NOT_SUPPORTED_ARRAY_POSITION_DOES_SUPPORT_ARRAYS_WITH_NULL=array_position does not support arrays with elements that are null or contain null
NOT_SUPPORTED_ARRAY_REMOVE_DOES_SUPPORT_ARRAYS_WITH_NULL=array_remove does not support arrays with elements that are null or contain null
NOT_SUPPORTED_ARRAY_WITH_ELEMENTS_FOR_COMPARISON=Array contains elements not supported for comparison
NOT_SUPPORTED_TOO_MANY_ARGUMENTS_FOR_CONCATENATION=Too many arguments for string concatenation
NOT_SUPPORTED_ALGORITHM_FOR_OS=%s is not supported in your OS
NOT_SUPPORTED_PLAN_TYPE_FOR_JSON=Unsupported explain plan type %s for JSON format
NOT_SUPPORTED_TOO_MANY_ARGUMENTS_FOR_LAMBDA_EXPRESSION=Too many arguments for lambda expression
NOT_SUPPORTED_TOO_MANY_ARGUMENTS_FOR_VARARG_FUNCTION=Too many arguments for vararg function
NOT_SUPPORTED_SESSION_PROPERTY_NOT_SET_TO_ENABLE_EXCHANGE_MATERIALIZATION=The \partitioning_provider_catalog\" session property must be set to enable the exchanges materialization. The catalog must support providing a custom partitioning and storing temporary tables.
NOT_SUPPORTED_TEMPORARY_TABLE_CREATION=Temporary table cannot be created in catalog "%s": %s
NOT_SUPPORTED_NOT_IMPLEMENTED_YET=not yet implemented: %s
NOT_SUPPORTED_DYNAMIC_FILTERING_WITH_GROUP_EXECUTION=Dynamic filtering cannot be used with grouped execution
NOT_SUPPORTED_CREATE_TABLE_IF_NOT_EXISTS=CREATE TABLE IF NOT EXISTS is not supported in this context %s
NOT_SUPPORTED_INSERT_WITHOUT_ALL_DISTRIBUTION_COLUMNS=INSERT must write all distribution columns: %s
NOT_SUPPORTED_TABLE-WIDE_STATISTIC_TYPE=Table-wide statistic type not supported: %s
NOT_SUPPORTED_OFFSET_SUPPORT_NOT_ENABLED=Offset support is not enabled
NOT_SUPPORTED_CATALOG_USED_AS_PARTITIONING_PROVIDER=Catalog "%s" cannot be used as a partitioning provider: %s
NOT_SUPPORTED_DATA_TYPE_IN_EXPLAIN=Unsupported data type in EXPLAIN (TYPE IO): %s
NOT_SUPPORTED_EXTERNAL_FUNCTION_IN_JOIN_FILTER=External function in join filter is not supported: %s
NOT_SUPPORTED_RENAME_COLUMN_IN_CATALOG=Rename column not supported in catalog: '%s'
NOT_SUPPORTED_TABLE_RENAME_ACROSS_SCHEMA=Table rename across schemas is not supported in Oracle
NOT_SUPPORTED_TYPE_FIXED_LEN_BYTE_ARRAY=type FIXED_LEN_BYTE_ARRAY supported as DECIMAL; got %s
NOT_SUPPORTED_PARQUET_TYPE=Unsupported parquet type: %s
NOT_SUPPORTED_PARQUET_TIMESTAMP_NOT_12_BYTES=Parquet timestamp must be 12 bytes, actual %d
NOT_SUPPORTED_PRIMITIVE_TYPE=Unsupported primitive type: %s
NOT_SUPPORTED_PARQUET_WRITER=Unsupported type for Parquet writer: %s
NOT_SUPPORTED_PARQUET_WRITER_VERSION=Unsupported Parquet writer version: %s
NOT_SUPPORTED_SETTING_WEIGHTS_BY_SCHEDULER=This scheduler does not support setting weights
NOT_SUPPORTED_ROUTER_SCHEDULER=Unsupported router scheduler type %s
NOT_SUPPORTED_DELETE_QUERY_ON_SPARK=delete queries are not supported by presto on spark
NOT_SUPPORTED_PAGE_SINK_COMMIT_IN_CATALOG=catalog does not support page sink commit: %s
NOT_SUPPORTED_AUTOMATIC_WRITER_ON_SPARK=Automatic writers scaling is not supported by Presto on Spark
NOT_SUPPORTED_ORDER_SENSITIVE_EXCHANGE_ON_SPARK=Order sensitive exchange is not supported by Presto on Spark. fragmentId: %s, sourceFragmentIds: %s
NOT_SUPPORTED_CONNECTOR_ANALYZE=This connector does not support analyze
NOT_SUPPORTED_CONNECTOR_CUSTOM_PARTITIONING=This connector does not support custom partitioning
NOT_SUPPORTED_CONNECTOR_CREATE_SCHEMA=This connector does not support creating schemas
NOT_SUPPORTED_CONNECTOR_DROP_SCHEMA=This connector does not support dropping schemas
NOT_SUPPORTED_CONNECTOR_RENAME_SCHEMA=This connector does not support renaming schemas
NOT_SUPPORTED_CONNECTOR_CREATE_TABLE=This connector does not support creating tables
NOT_SUPPORTED_CREATE_TEMPORARY_TABLE=This connector does not support creating temporary tables
NOT_SUPPORTED_CONNECTOR_DROP_TABLE=This connector does not support dropping tables
NOT_SUPPORTED_CONNECTOR_TRUNCATE_TABLE=This connector does not support truncating tables
NOT_SUPPORTED_CONNECTOR_RENAME_TABLE=This connector does not support renaming tables
NOT_SUPPORTED_CONNECTOR_ADDING_COLUMNS=This connector does not support adding columns
NOT_SUPPORTED_CONNECTOR_RENAME_COLUMNS=This connector does not support renaming columns
NOT_SUPPORTED_CONNECTOR_DROP_COLUMNS=This connector does not support dropping columns
NOT_SUPPORTED_TABLE_WITH_MULTIPLE_LAYOUTS=Tables with multiple layouts can not be written
NOT_SUPPORTED_CONNECTOR_CREATE_TABLE_WITH_DATA=This connector does not support creating tables with data
NOT_SUPPORTED_CONNECTOR_INSERT=This connector does not support inserts
NOT_SUPPORTED_CONNECTOR_UPDATES_OR_DELETES=This connector does not support updates or deletes
NOT_SUPPORTED_CONNECTOR_DELETES=This connector does not support deletes
NOT_SUPPORTED_CONNECTOR_CREATE_VIEWS=This connector does not support creating views
NOT_SUPPORTED_CONNECTOR_DROP_VIEWS=This connector does not support dropping views
NOT_SUPPORTED_CONNECTOR_CREATE_MATERIALIZED_VIEWS=This connector does not support creating materialized views
NOT_SUPPORTED_CONNECTOR_DROP_MATERIALIZED_VIEWS=This connector does not support dropping materialized views
NOT_SUPPORTED_CONNECTOR_GET_MATERIALIZED_VIEW_STATUS=This connector does not support getting materialized views status
NOT_SUPPORTED_CONNECTOR_REFRESH_MATERIALIZED_VIEW=This connector does not support refresh materialized views
NOT_SUPPORTED_CONNECTOR_CREATE_ROLES=This connector does not support create role
NOT_SUPPORTED_CONNECTOR_DROP_ROLES=This connector does not support drop role
NOT_SUPPORTED_CONNECTOR_ROLES=This connector does not support roles
NOT_SUPPORTED_CONNECTOR_GRANT=This connector does not support grants
NOT_SUPPORTED_CONNECTOR_REVOKE=This connector does not support revokes
NOT_SUPPORTED_CONNECTOR_PAGE_SINK_COMMIT=This connector does not support page sink commit
NOT_SUPPORTED_CONNECTOR_METADATA_UPDATE_REQUESTS=This connector does not support metadata update requests
NOT_SUPPORTED_CONNECTOR_METADATA_UPDATE_CLEANUP=This connector does not support metadata update cleanup
NOT_SUPPORTED_ORDER_BY_LIMITS=ORDER BY LIMIT > %s is not supported
NOT_SUPPORTED_COLUMN_TYPE=Unsupported column type %s
INVALID_SESSION_PROPERTY_VALUE_NOT_IN_RANGE[0,1.0]=%s must be > 0 and <= 1.0: %s
INVALID_SESSION_PROPERTY_SIZE_LESS_THAN_ONE=%s must be greater than 0: %s
INVALID_SESSION_PROPERTY_NEGATIVE_VIRTUAL_BUCKET_COUNT=%s must not be negative: %s
INVALID_SESSION_PROPERTY_VALUE_NOT_IN_RANGE[0,100.0]=%s must be between 0.0 and 100.0 inclusive: %s
INVALID_SESSION_PROPERTY_UNKNOWN_SESSION_PROPERTY=Unknown session property: %s.%s
INVALID_SESSION_PROPERTY_VALUE_NOT_TWO_OR_MORE=%s must be greater than or equal to 2: %s
INVALID_SESSION_PROPERTY_NO_SPILL_PATH_CONFIGURED=%s cannot be set to true; no spill paths configured
INVALID_SESSION_PROPERTY_NOT_POWER_OF_2=%s must be a power of 2: %s
INVALID_SESSION_PROPERTY_VALUE_NULL=%s must be non-null
INVALID_SESSION_PROPERTY_VALUE_LESS_THAN_LOWERBOUND=%s must be equal or greater than %s
INVALID_SESSION_PROPERTY_UNABLE_TO_SET_SESSION_PROPERTY=Unable to set session property '%s' to '%s': %s
INVALID_SESSION_PROPERTY_UNKNOWN_CONNECTOR=Unknown connector %s
INVALID_SESSION_PROPERTY_UNKNOWN_SESSION_PROPERTY_NAME=Unknown session property %s
INVALID_SESSION_PROPERTY_TYPE_NOT_EQUAL_TO_REQUESTED_TYPE=Property %s is type %s, but requested type was %s
INVALID_SESSION_PROPERTY_INVALID_PROPERTY=%s is invalid: %s
INVALID_SESSION_PROPERTY_SESSION_PROPERTY_NULL=Session property value must not be null
INVALID_SESSION_PROPERTY_SESSION_PROPERTY_NOT_SUPPORTED=Session property type %s is not supported
INVALID_SESSION_PROPERTY_SESSION_PROPERTY_KEY_TYPE_NOT_SUPPORTED=Session property map key type %s is not supported
INVALID_SESSION_PROPERTY_UNABLE_TO_PARSE_SESSION_PROPERTY=Unable to parse session property: %s
CONSTRAINT_VIOLATION_NULL_VALUED_NOT_ALLOWED=NULL value not allowed for NOT NULL column: %s
TRANSACTION_CONFLICT_TABLE-ALREADY_EXISTS=Table already exists with a different schema: '%s'
TRANSACTION_CONFLICT_DROP_AND_RECREATE_SAME_TABLE=Dropping and then recreating the same table in a transaction is not supported
TRANSACTION_CONFLICT_TABLE_DROPPED_BY_OTHER_TRANSACTION=Table %s.%s was dropped by another transaction
TRANSACTION_CONFLICT_ANOTHER_TRANSACTION_CREATED_PARTITION=Another transaction created partition %s in table %s.%s
TRANSACTION_CONFLICT_OPERATION_ON_SAME_PARTITION_WITH_DIFFERENT_USER=Operation on the same partition with different user in the same transaction is not supported
TRANSACTION_CONFLICT_PARTITION_MODIFIED_BY_A_TRANSACTION_DELETED_BY_ANOTHER_TRANSACTION=The partition that this transaction modified was deleted in another transaction. %s %s
INVALID_TABLE_PROPERTY_KEY/VALUE_TYPE_NOT_PLAIN_TYPE=Key/value types of a MAP column must be plain types
INVALID_TABLE_PROPERTY_DUPLICATE_COLUMN_NAME=Duplicate column names are not supported
INVALID_TABLE_PROPERTY_DUPLICATE_COLUMN_FAMILY_PAIR_DETECTED_IN_MAPPING=Duplicate column family/qualifier pair detected in column mapping, check the value of %s
INVALID_TABLE_PROPERTY_COLUMN_FAMILY_MAPPING_RESERVED=Column family/qualifier mapping of %s:%s is reserved
INVALID_TABLE_PROPERTY_COLUMN_GENERATION_FOR_EXTERNAL_TABLE_NOT_SUPPORTED=Column generation for external tables is not supported, must specify %s
INVALID_TABLE_PROPERTY_ROW_ID_IN_LOCALITY_GROUP=Row ID column cannot be in a locality group
INVALID_TABLE_PROPERTY_UNKNOWN_PRESTO_COLUMN_DEFINED_IN_LOCALITY_GROUP=Unknown Presto column defined for locality group %s
INVALID_TABLE_PROPERTY_LOCALITY_GROUP_STRING_MALFORMED=Locality groups string is malformed. See documentation for proper format.
INVALID_TABLE_PROPERTY_DISTRIBUTED_COLUMN_NOT_DEFINED=Distribute columns not defined on table: %s
INVALID_TABLE_PROPERTY_PROPERTY_LESS_THAN_ZERO=%s property is negative
INVALID_TABLE_PROPERTY_ALL_PROPERTIES_NOT_SET=All properties [%s, %s, %s] must be set if any are set
INVALID_TABLE_PROPERTY_PROPERTY_REQUIRED_FOR_TABLE_ENGINE=The property of %s is required for table engine %s
INVALID_TABLE_PROPERTY_BUCKETING_COLUMN_NOT_PRESENT=Bucketing columns %s not present in schema
INVALID_TABLE_PROPERTY_CANNOT_SPECIFY_TABLE_PROPERTY_FOR_STORAGE_FORMAT=Cannot specify %s table property for storage format: %s
INVALID_TABLE_PROPERTY_CANNOT_LOCATION_AVRO_SCHEMA_FILE=Cannot locate Avro schema file: %s
INVALID_TABLE_PROPERTY_NOT_VALID_FILE_SYSTEM_URI=Avro schema file is not a valid file system URI: %s
INVALID_TABLE_PROPERTY_CANNOT_OPEN_FILE=Cannot open Avro schema file: %s
INVALID_TABLE_PROPERTY_EXTERNAL_LOCATION_NOT_A_DIRECTORY=External location must be a directory
INVALID_TABLE_PROPERTY_EXTERNAL_LOCATION_NOT_A_VALID_FILE_SYSTEM_URI=External location is not a valid file system URI: %s
INVALID_TABLE_PROPERTY_PARTITION-FORMAT_DOES-NOT_MATCH_TABLE_FORMAT=For encrypted tables, partition format (%s) should match table format (%s). Using the session property %s or appropriately setting %s can help with ensuring this
INVALID_TABLE_PROPERTY_SPECIFYING_LOCATION_FOR_MATERIALIZED_VIEW_NOT_SUPPORTED=Specifying external location for materialized view is not supported.
INVALID_TABLE_PROPERTY_BUCKETING_COLUMN_NOT_PRESENT_IN_SCHEMA=Bucketing columns %s not present in schema
INVALID_TABLE_PROPERTY_SORTING_COLUMN_NOT_PRESENT_IN_SCHEMA=Sorting columns %s not present in schema
INVALID_TABLE_PROPERTY_PARTITION_COLUMN_NOT_PRESENT_IN_SCHEMA=Partition columns %s not present in schema
INVALID_TABLE_PROPERTY_TABLE_CONTAINS_ONLY_PARTITION_COLUMNS=Table contains only partition columns
INVALID_TABLE_PROPERTY_BOTH_TABLE_ENCRYPTION_REFERENCE_AND_COLUMN_ENCRYPTION_INFO_SPECIFIED=Only one of %s or %s should be specified
INVALID_TABLE_PROPERTY_PARTITION_COLUMN_SPECIFIED_AS_ENCRYPTION_COLUMN=Partition column (%s) cannot be used as an encryption column
INVALID_TABLE_PROPERTY_UNABLE_TO_FIND_COLUMN=In %s unable to find column %s
INVALID_TABLE_PROPERTY_2_ENCRYPTION_KEYS_IN_SAME_COLUMN=The same column/subfield cannot have 2 encryption keys
INVALID_TABLE_PROPERTY_ENCRYPT_COLUMN_SUBFIELD_DECLARED_IN_COLUMN=In %s subfields declared in %s, but %s has type %s
INVALID_TABLE_PROPERTY_FOUND_KEYREFERENCE_AT_PARENT_PATH_FOR_COLUMN_WITH_SUBFIELD=For (%s) found a keyReference at a higher level field (%s)
INVALID_TABLE_PROPERTY_ENCRYPTION_ALGORITHM_NULL=%s needs to be provided for DWRF encrypted tables
INVALID_TABLE_PROPERTY_SORTED_BY_PROPERTY_SPECIFIED_EVEN_WITHOUT_BUCKETED_BY_PROPERTY=%s may be specified only when %s is specified
INVALID_TABLE_PROPERTY_BUCKET_COUNT_PROPERTY_LESS_THAN_ZERO=%s must be greater than zero
INVALID_TABLE_PROPERTY_BUCKET_COUNT_PROPERTY_MORE_THAN_LIMIT=%s should be no more than 1000000
INVALID_TABLE_PROPERTY_BUCKETED_BY_PROPERTY_AND_BUCKET_COUNT_PROPERTY_NOT_SPECIFIED_TOGETHER=%s and %s must be specified together
INVALID_TABLE_PROPERTY_KEY_NOT_A_SINGLE_CHARACTER_STRING=%s must be a single character string, but was: '%s'
INVALID_TABLE_PROPERTY_PREFERRED_ORDERING_COLUMN_SPECIFIED_WITH_BUCKETED_BY_PROPERTY=%s must not be specified when %s is specified
INVALID_TABLE_PROPERTY_ENCRYPTED_COLUMN_PROPERTY_NULL=Encrypted columns property cannot have null value
INVALID_TABLE_PROPERTY_ENCRYPTED_COLUMN_ENTRY_NOT_IN_FORMAT=Encrypted column entry needs to be in the format 'key1:col1,col2'. Received: %s
INVALID_TABLE_PROPERTY_COLUMN_ASSIGNED_2_KEY_REFERENCE=Column %s has been assigned 2 key references (%s and %s). Only 1 is allowed
INVALID_TABLE_PROPERTY_FAILED_TO_CONVERT_OBJECT_TYPE_TO_EXPRESSION_VALUE=Failed to convert object of type %s to expression: %s
NUMERIC_VALUE_OUT_OF_RANGE_DATASIZE_BYTES=Value out of range: '%s' ('%sB')
NUMERIC_VALUE_OUT_OF_RANGE_CANNOT_CAST_TO_INTEGER=Cannot cast '%s' to %s
NUMERIC_VALUE_OUT_OF_RANGE_VALUE_OUT_RANGE_FOR_ABS(TINYINT)=Value -128 is out of range for abs(tinyint)
NUMERIC_VALUE_OUT_OF_RANGE_VALUE_OUT_RANGE_FOR_ABS(SMALLINT)=Value -32768 is out of range for abs(smallint)
NUMERIC_VALUE_OUT_OF_RANGE_VALUE_OUT_RANGE_FOR_ABS(INTEGER)=Value -2147483648 is out of range for abs(integer)
NUMERIC_VALUE_OUT_OF_RANGE_VALUE_OUT_RANGE_FOR_ABS(BIGINT)=Value -9223372036854775808 is out of range for abs(bigint)
NUMERIC_VALUE_OUT_OF_RANGE_BUCKET_FOR_VALUE_OUT_OF_RANGE=Bucket for value %s is out of range
NUMERIC_VALUE_OUT_OF_RANGE_DECIMAL_OVERFLOW_NUM=decimal overflow: %s
NUMERIC_VALUE_OUT_OF_RANGE_BIGINT_ADDITION_OVERFLOW=bigint addition overflow: %s + %s
NUMERIC_VALUE_OUT_OF_RANGE_BIGINT_SUBTRACTION_OVERFLOW=bigint subtraction overflow: %s - %s
NUMERIC_VALUE_OUT_OF_RANGE_BIGINT_MULTIPLICATION_OVERFLOW=bigint multiplication overflow: %s * %s
NUMERIC_VALUE_OUT_OF_RANGE_BIGINT_DIVISION_OVERFLOW=bigint division overflow: %s / %s
NUMERIC_VALUE_OUT_OF_RANGE_BIGINT_NEGATION_OVERFLOW=bigint negation overflow: %s
NUMERIC_VALUE_OUT_OF_RANGE_FOR_INTEGER=Out of range for integer: %s
NUMERIC_VALUE_OUT_OF_RANGE_FOR_SMALLINT=Out of range for smallint: %s
NUMERIC_VALUE_OUT_OF_RANGE_FOR_TINYINT=Out of range for tinyint: %s
NUMERIC_VALUE_OUT_OF_RANGE_DECIMAL_OVERFLOW=Decimal overflow
NUMERIC_VALUE_OUT_OF_RANGE_INTEGER_ADDITION_OVERFLOW=integer addition overflow: %s + %s
NUMERIC_VALUE_OUT_OF_RANGE_INTEGER_SUBTRACTION_OVERFLOW=integer subtraction overflow: %s - %s
NUMERIC_VALUE_OUT_OF_RANGE_INTEGER_MULTIPLICATION_OVERFLOW=integer multiplication overflow: %s * %s
NUMERIC_VALUE_OUT_OF_RANGE_INTEGER_NEGATION_OVERFLOW=integer negation overflow: %s
NUMERIC_VALUE_OUT_OF_RANGE_SMALLINT_ADDITION_OVERFLOW=smallint addition overflow: %s + %s
NUMERIC_VALUE_OUT_OF_RANGE_SMALLINT_SUBTRACTION_OVERFLOW=smallint subtraction overflow: %s - %s
NUMERIC_VALUE_OUT_OF_RANGE_SMALLINT_MULTIPLICATION_OVERFLOW=smallint multiplication overflow: %s * %s
NUMERIC_VALUE_OUT_OF_RANGE_SMALLINT_NEGATION_OVERFLOW=smallint negation overflow: %s
NUMERIC_VALUE_OUT_OF_RANGE_TINYINT_ADDITION_OVERFLOW=tinyint addition overflow: %s + %s"
NUMERIC_VALUE_OUT_OF_RANGE_TINYINT_SUBTRACTION_OVERFLOW=tinyint subtraction overflow: %s - %s
NUMERIC_VALUE_OUT_OF_RANGE_TINYINT_MULTIPLICATION_OVERFLOW=tinyint multiplication overflow: %s * %s
NUMERIC_VALUE_OUT_OF_RANGE_TINYINT_NEGATION_OVERFLOW=tinyint negation overflow: %s
UNKNOWN_TRANSACTION_UNKNOWN_TRANSACTION_ID=Unknown transaction ID: %s. Possibly expired? Commands ignored until end of transaction block
NOT_IN_TRANSACTION_NO_TRANSACTION_IN_PROGRESS=No transaction in progress
TRANSACTION_ALREADY_ABORTED_CURRENT_TRANSACTION_ABORTED=Current transaction is aborted, commands ignored until end of transaction block
TRANSACTION_ALREADY_ABORTED_CURRENT_TRANSACTION_ALREADY_ABORTED=Current transaction has already been aborted
READ_ONLY_VIOLATION_CANNOT_EXECUTE_WRITE=Cannot execute write in a read-only transaction
MULTI_CATALOG_WRITE_CONFLICT_NOT_SUPPORTED=Multi-catalog writes not supported in a single transaction. Attempt write to catalog %s, but already wrote to catalog %s
AUTOCOMMIT_WRITE_CONFLICT_CATALOG_DOES_NOT_SUPPORT_WRITE_WITHOUT_AUTOCOMMIT=Catalog %s only supports writes using autocommit
UNSUPPORTED_ISOLATION_LEVEL_DOES_NOT_MEET_LEVEL=Connector supported isolation level %s does not meet requested isolation level %s
INCOMPATIBLE_CLIENT_DOES_NOT_SUPPORT_TRANSACTION=Client does not support transactions
SUBQUERY_MULTIPLE_ROWS_RETURNED_BY_SCALAR_SUBQUERY=Scalar sub-query has returned multiple rows
PROCEDURE_NOT_FOUND_NOT_REGISTERED=Procedure not registered: %s
INVALID_PROCEDURE_ARGUMENT_INPUT_PARTITION_COLUMN_NAME_DOES_NOT_MATCH_WITH_ACTUAL_PARTITION_COLUMN_NAME=input partition column names does not match actual partition column names
INVALID_PROCEDURE_ARGUMENT_TABLE_NOT_PARTITIONED=Table is not partitioned: %s
INVALID_PROCEDURE_ARGUMENT_INVALID_METADATA_SYNC_MODE=Invalid partition metadata sync mode: %s
INVALID_PROCEDURE_ARGUMENT_NULL=Procedure argument cannot be null: %s
QUERY_REJECTED_TABLE_HAS_NO_RANGE_PARTITION=Table %s has no range partition
AMBIGUOUS_FUNCTION_CALL_FUNCTION_HAS_MULTIPLE_SIGNATURES=Function '%s' has multiple signatures: %s. Please specify parameter types.
AMBIGUOUS_FUNCTION_CALL_FOR_IMPLEMENTATION=Ambiguous function call (%s) for %s
INVALID_SCHEMA_PROPERTY_INAVLID_LOCATION_URI=Invalid location URI: %s
SCHEMA_NOT_EMPTY_HAS_SCHEMA_NAME=Schema not empty: %s
QUERY_TEXT_TOO_LARGE_EXCEEDS_MAX_LENGTH=Query text length (%s) exceeds the maximum length (%s)
UNSUPPORTED_SUBQUERY_GIVEN_CORRELATED_SUBQUERY=Given correlated subquery is not supported
ADMINISTRATIVELY_KILLED_QUERY_KILLED=Query killed. %s No message provided. Message: %s
QUERY_HAS_TOO_MANY_STAGES_QUERY_KILLED=Query killed because the cluster is overloaded with too many tasks (%s) and this query was running with the highest number of tasks (%s). %s Otherwise, please try again later.
QUERY_HAS_TOO_MANY_STAGES_NUMBER_EXCEED_ALLOWED_MAXIMUM=Number of stages in the query (%s) exceeds the allowed maximum (%s).
INVALID_SPATIAL_PARTITIONING_TABLE_NOT_FOUND=Table not found: %s
INVALID_SPATIAL_PARTITIONING_INVALID_NAME=Invalid name: %s
INVALID_ANALYZE_PROPERTY_INVALID_NULL_VALUE=Invalid null value in analyze partitions property
INVALID_ANALYZE_PROPERTY_NOT_A_PARTITIONED_TABLE=Only partitioned table can be analyzed with a partition list
WARNING_AS_ERROR_WARNING_HANDLING_LEVEL_AS_ERROR=Warning handling level set to AS_ERROR. Warnings: %n %s
INVALID_ARGUMENTS_ELASTICSEARCH_QUERY_NOT_BASE32-ENCODED=Elasticsearch query for '%s' is not base32-encoded correctly
INVALID_ARGUMENTS_ELASTICSEARCH_QUERY_NOT_VALID_JSON=Elasticsearch query for '%s' is not valid JSON
INVALID_ARGUMENTS_CONNECTORCOMMITHANDLE_EXPECTED=Type ConnectorCommitHandle is expected
INVALID_ARGUMENTS_TYPE_NOT_ALLOW_ORDERING=Type %s does not allow ordering
EXCEEDED_PLAN_NODE_LIMIT_NUMBER_OF_LEAF_NODE_EXCEEDS_THRESHOLD=Number of leaf nodes in logical plan exceeds threshold %s set in max_leaf_nodes_in_plan
GENERIC_INTERNAL_ERROR_CANNOT_CREATE_CACHE_DIRECTORY=cannot create cache directory %s
GENERIC_INTERNAL_ERROR_FAILED_TO_LOAD_DELTA_TABLE=Failed to load Delta table: %s
GENERIC_INTERNAL_ERROR_INVALID_INETADRESS_LENGTH=Invalid InetAddress length: %s
GENERIC_INTERNAL_ERROR_USERDEFINEDTYPE=Error getting UserDefinedType: %s
GENERIC_INTERNAL_ERROR_NEITHER_ENCRYPTCOLUMN_OR_ENCRYPTTABLE_PRESENT=Neither of encryptColumn or encryptTable present. We should never hit this
GENERIC_INTERNAL_ERROR_HAVE_BOTH_TABLE_AND_COLUMN_LEVEL_SETTINGS=Cannot have both table and column level settings. Given: %s
GENERIC_INTERNAL_ERROR_RECORDCURSOR_INTERRUPTED=RecordCursor was interrupted
GENERIC_INTERNAL_ERROR_COLUMN_ENCRYPTION_INFORMATION_EMPTY=columnEncryptionInformation cannot be empty
GENERIC_INTERNAL_ERROR_NO_PARTITION_IN_LAYOUT=Layout does not contain partitions
GENERIC_INTERNAL_ERROR_BUCKETHANDLE_NOT_PRESENT=SchedulingPolicy is bucketed, but BucketHandle is not present
GENERIC_INTERNAL_ERROR_READBUCKETCOUNT_MORE_THAN_TABLEBUCKETCOUNT=readBucketCount (%s) is greater than the tableBucketCount (%s) which generally points to an issue in plan generation
GENERIC_INTERNAL_ERROR_NOT_EXPECTED_PARTITION_VALUE=Expected %s partitions but found %s
GENERIC_INTERNAL_ERROR_PARTITION_NOT_LOADED=Partition not loaded: %s
GENERIC_INTERNAL_ERROR_COMPRESSIONCODEC_NOT_FOUND=Failed to find compressionCodec for inputFormat: %s
GENERIC_INTERNAL_ERROR_CACHING_FILE_SYSTEM_CREATED=cannot create caching file system
GENERIC_INTERNAL_ERROR_NOT_SUPPORTED_COMPRESSION=%s compression is not supported for %s
GENERIC_INTERNAL_ERROR_UNENFORCED_FILTER_FOUND=Unenforced filter found %s but not handled
GENERIC_INTERNAL_ERROR_PROVIDED_NULL_COLUMN_NAME=Cannot provide null column name for encryption columns
GENERIC_INTERNAL_ERROR_ROW_VALUE_FIELD_COUNT_DOES_NOT_MATCH=Expected row value field count does not match type field count
GENERIC_INTERNAL_ERROR_FAILED_TO_SCAN_FILES=failed to scan files for delete
GENERIC_INTERNAL_ERROR_INVALID_PARTITION_TYPE=Invalid partition type %s
GENERIC_INTERNAL_ERROR_UNKNOWN_COLUMN_TYPE=Unknown ColumnType: %s
GENERIC_INTERNAL_ERROR_LEADER_ELECTION_IN_PROGRESS_FOR_KAFKA=Leader election in progress for Kafka topic '%s' partition %s
GENERIC_INTERNAL_ERROR_COULD_NOT_PARSE_THE_AVRO_SCHEMA=Could not parse the Avro schema at: %s
GENERIC_INTERNAL_ERROR_UNKOWN_COMPRESSION_ALGORITHM=Unknown compression algorithm %s for column %s
GENERIC_INTERNAL_ERROR_UNKNOWN_ENCODING_FOR_COLUMN=Unknown encoding %s for column %s
GENERIC_INTERNAL_ERROR_NOT_SINGLE_COORDINATOR=Distribution for dynamic system table must be %s
GENERIC_INTERNAL_ERROR_DUPLICATE_COLUMN_NAME=Duplicate column name: %s
GENERIC_INTERNAL_ERROR_COLUMN_DOES_NOT_EXIST=Column does not exist: %s.%s
GENERIC_INTERNAL_ERROR_QUERY_FAILED=Query failed for an unknown reason
GENERIC_INTERNAL_ERROR_INVALID_TABLE_ELEMENT=Invalid TableElement: %s
GENERIC_INTERNAL_ERROR_QUERY_ALREADY_REGISTERED=Query %s already registered
GENERIC_INTERNAL_ERROR_TASK_FAILED=A task failed for an unknown reason
GENERIC_INTERNAL_ERROR_TASK_IN_ABORTED_STATE=A task is in the ABORTED state but stage is %s
GENERIC_INTERNAL_ERROR_QUERY_STAGE_ABORTED=Query stage was aborted
GENERIC_INTERNAL_ERROR_STAGE_EXECUTION_NOT_DONE=Scheduling is complete, but stage execution %s is in state %s
GENERIC_INTERNAL_ERROR_DRIVER_INTERRUPTED=Driver was interrupted
GENERIC_INTERNAL_ERROR_UNSUPPORTED_TASK=unsupported task result client scheme %s
GENERIC_INTERNAL_ERROR_OPERATOR_HAS_NON-ZERO_SYSTEM_MEMORY_AFTER_DESTROY=Operator %s has non-zero system memory (%d bytes) after destroy()
GENERIC_INTERNAL_ERROR_OPERATOR_HAS_NON-ZERO_USER_MEMORY_AFTER_DESTROY=Operator %s has non-zero user memory (%d bytes) after destroy()
GENERIC_INTERNAL_ERROR_OPERATOR_HAS_NON-ZERO_SYSTEM_REVOCABLE_AFTER_DESTROY=Operator %s has non-zero revocable memory (%d bytes) after destroy()
GENERIC_INTERNAL_ERROR_IN_RUN_LISTENER=Exception while running the listener
GENERIC_INTERNAL_ERROR_WHEN_LOADING_INDEX_FOR_JOIN=Error loading index for join
GENERIC_INTERNAL_ERROR_WHILE_CASTING_ARRAY_ELEMENT=Error casting array element to VARCHAR
GENERIC_INTERNAL_ERROR_INVALID_COLOR_INDEX=Invalid color index: %s
GENERIC_INTERNAL_ERROR_UNABLE_TO_FIND_ERROR_FOR_CODE=Unable to find error for code: %s
GENERIC_INTERNAL_ERROR_NULL_RESPONSE=Request failed with null response
GENERIC_INTERNAL_ERROR_REQUEST_FAILED=Request failed with HTTP status %s
GENERIC_INTERNAL_ERROR_ENCRYPTION_DATA_TOO_SMALL=Encrypted data size is too small: %s. It must be at least the size of the initialization vector: %s.
GENERIC_INTERNAL_ERROR_CANNOT_DECRYPT_DATA=Cannot decrypt previously encrypted data: %s
GENERIC_INTERNAL_ERROR_FAILED_TO_ENCRYPT_DATA=Failed to encrypt data: %s
GENERIC_INTERNAL_ERROR_SPILL_CYPHER_ALREADY_DESTROYED=Spill cipher already destroyed
GENERIC_INTERNAL_ERROR_FAILED_TO_INITIALIZE_SPILL_FOR_ENCRYPTION=Failed to initialize spill cipher for encryption: %s
GENERIC_INTERNAL_ERROR_FAILED_TO_INITIALIZE_SPILL_FOR_DECRYPTION=Failed to initialize spill cipher for decryption: %s
GENERIC_INTERNAL_ERROR_FAILED_TO_CREATE_SPILL_CIPHER=Failed to create spill cipher: %s
GENERIC_INTERNAL_ERROR_FAILED_TO_GENERATE_NEW_SECRET_KEY=Failed to generate new secret key: %s
GENERIC_INTERNAL_ERROR_FAILED_TO_CREATE_SPILL_SINK=Failed to create spill sink
GENERIC_INTERNAL_ERROR_FAILED_TO_COMMIT_SPILL_FILE=Failed to commit spill file
GENERIC_INTERNAL_ERROR_FAILED_TO_READ_SPILLED_PAGES=Failed to read spilled pages
GENERIC_INTERNAL_ERROR_FAILED_TO_CLOSE_SPILLER=Failed to close spiller
GENERIC_INTERNAL_ERROR_FAILED_TO_SPILL_PAGES=Failed to spill pages
GENERIC_INTERNAL_ERROR_QUERY_DOES)NOT_PARSE=Formatted query does not parse: %s
GENERIC_INTERNAL_ERROR_QUERY_DOES_NOT_ROUND-TRIP=Query does not round-trip: %s
GENERIC_INTERNAL_ERROR_CANNOT_ASSIGN_PLAN_ID_TO_JOIN_NODE=Cannot assign canonical plan id to Canonical join node: %s
GENERIC_INTERNAL_ERROR_CANNOT_ASSIGN_PLAN_ID_TO_TABLE_SCAN_NODE=Cannot assign canonical plan id to Canonical table scan node: %s
GENERIC_INTERNAL_ERROR_UNKNOWN_PLAN_PHASE=Unknown plan phase %s
GENERIC_INTERNAL_ERROR_MIX_SPHERICAL_AND_EUCLIDEAN_GEOMETRIC_TYPES=Mixing spherical and euclidean geometric types
GENERIC_INTERNAL_ERROR_UNEXPECTED_CALL_REPLACECHILDREN=Unexpected call replaceChildren for %s
GENERIC_INTERNAL_ERROR_CANNOT_ASSIGN_PLAN_ID=Cannot assign canonical plan id for: %s
GENERIC_INTERNAL_ERROR_CANNOT_ASSIGN_PLAN_ID_TO_GROUP_REFERENCE_NODE=Cannot assign canonical plan id to Group Reference node: %s
GENERIC_INTERNAL_ERROR_UNEXPECTED_PLAN_NODE=Unexpected plan node between partial and final aggregation
GENERIC_INTERNAL_ERROR_CODEPOINT_TYPE_CANNOT_BE_SERIALIZED=CodePoints type cannot be serialized
GENERIC_INTERNAL_ERROR_REGEXP_TYPE_CANNOT_BE_SERIALIZED=RegExp type cannot be serialized
GENERIC_INTERNAL_ERROR_JSONPATH_TYPE_CANNOT_BE_SERIALIZED=JsonPath type cannot be serialized
GENERIC_INTERNAL_ERROR_LIKEPATTERN_CANNOT_BE_SERIALIZED=LikePattern type cannot be serialized
GENERIC_INTERNAL_ERROR_ROW_FILED_COUNT_AND_TYPE_FIELD-COUNT_DONT_MATCH=Expected row value field count does not match type field count
GENERIC_INTERNAL_ERROR_UNHANDLED_TYPE=Unhandled type for %s : %s
GENERIC_INTERNAL_ERROR_UNHANDLED_TYPE_FOR_SLICE=Unhandled type for Slice: %s
GENERIC_INTERNAL_ERROR_UNHANDLED_TYPE_FOR_BLOCK=Unhandled type for Block: %s
GENERIC_INTERNAL_ERROR_PIONT_TABLE_HANDLE_EXPECTED=Expected to find the pinot table handle for the scan node
GENERIC_INTERNAL_ERROR_MAP_CONTAINS_NULL_KEYS=Map must never contain null keys
GENERIC_INTERNAL_ERROR_AVRO_RECORD_NOT_FOUND=No avro record found
GENERIC_INTERNAL_ERROR_EXTRA_RECORD_FOUND=Unexpected extra record found
GENERIC_INTERNAL_ERROR_DECODING_AVRO_RECORD_FAILED=Decoding Avro record failed.
GENERIC_INTERNAL_ERROR_UNSUPPORTED_HASH_ALGORITHM=unsupported hash algorithm
GENERIC_INTERNAL_ERROR_TASKRESULTFETCHER_CLOSED=TaskResultFetcher is closed with %s pages left in the buffer
GENERIC_INTERNAL_ERROR_NATIVE_TASK_FAILED=Native task failed for an unknown reason
GENERIC_INTERNAL_ERROR_GETCOMMONPARTITIONINGHANDLE_IMPLEMENTED_WITHOUT_GETALTERNATIVELAYOUT=ConnectorMetadata getCommonPartitioningHandle() is implemented without getAlternativeLayout()
GENERIC_INTERNAL_ERROR_GETSTATISTICSCOLLECTION_IMPLEMENTED_WITHOUT_GETSTATISTICSCOLLECTIONMETADATA=ConnectorMetadata getTableHandleForStatisticsCollection() is implemented without getStatisticsCollectionMetadata()
GENERIC_INTERNAL_ERROR_GETSTATISTICSCOLLECTIONMETADATA_IMPLEMENTED_WITHOUT_BEGINSTATISTICSCOLLECTION=ConnectorMetadata getStatisticsCollectionMetadata() is implemented without beginStatisticsCollection()
GENERIC_INTERNAL_ERROR_BEGINSTATISTICSCOLLECTION_IMPLEMENTED_WITHOUT_FINISHSTATISTICSCOLLECTION=ConnectorMetadata beginStatisticsCollection() is implemented without finishStatisticsCollection()
GENERIC_INTERNAL_ERROR_BEGINCREATETABLE_IMPLEMENTED_WITHOUT_FINISHCREATETABLE=ConnectorMetadata beginCreateTable() is implemented without finishCreateTable()
GENERIC_INTERNAL_ERROR_BEGININSERT_IMPLEMENTED_WITHOUT_FINISHINSERT=ConnectorMetadata beginInsert() is implemented without finishInsert()
GENERIC_INTERNAL_ERROR_FINISHREFRESHMATERIALIZEDVIEW_NOT_IMPLEMENTED=ConnectorMetadata finishRefreshMaterializedView() is not implemented
GENERIC_INTERNAL_ERROR_NOT_ONE_PLANNODE=Expect exactly 1 child PlanNode
TOO_MANY_REQUESTS_FAILED_TASK_FAILED=%s (%s %s - %s failures, failure duration %s, total failed request time %s)
PAGE_TOO_LARGE_REMOTE_PAGE=Remote page is too large
NO_NODES_AVAILABLE_SORTEDCANDIDATE_NULL_OR_EMPTY=sortedCandidates is null or empty for ModularHashingNodeProvider
NO_NODES_AVAILABLE_NO_NODES_AVAILABLE_TO_RUN_QUERY=No nodes available to run query
NO_NODES_AVAILABLE_NO_WORKER_NODES_AVAILABLE=No worker nodes available
COMPILER_ERROR_NO_LAMBDA_FUNCTION_INTERFCE=Lambda function interface is required to be annotated with FunctionalInterface
COMPILER_ERROR_NO_METHOD_WITH_NAME_APPLY_IN_INTERFACE=Expect to have exactly 1 method with name 'apply' in interface %s
COMPILER_ERROR_COMPILER_FAILED=Compiler failed
REMOTE_TASK_MISMATCH_ERROR_URI=%s (%s)
SERVER_SHUTTING_DOWN_EXECUTION_REJECTED=Server is shutting down
SERVER_SHUTTING_DOWN_QUERY_CANCELLED=Server is shutting down. Query %s has been cancelled
SERVER_SHUTTING_DOWN_TASK_CANCELLED=Server is shutting down. Task %s has been canceled
FUNCTION_IMPLEMENTATION_MISSING_SIGNATURE_NOT_FOUND=%s not found
FUNCTION_IMPLEMENTATION_MISSING_UNSUPPORTED_TYPE=Unsupported type parameters (%s) for %s
FUNCTION_IMPLEMENTATION_MISSING_UNSUPPORTED_ARRAY_ELEMENT_TYPE=Unsupported array element type for array_normalize function: %s
SERVER_STARTING_UP_SERVER_INITIALIZING=Presto server is still initializing
FUNCTION_IMPLEMENTATION_ERROR_SCAN_RETURNED_MORE_THAN_ONE_ENTRY=Scan for default tablet returned more than one entry
FUNCTION_IMPLEMENTATION_ERROR_LOADALL_CALLED_WITH_NON-HOMOGENEOUS_COLLECTION=loadAll called with a non-homogeneous collection of cache keys
FUNCTION_IMPLEMENTATION_ERROR_RECEIVED_MORE_THAN_ONE_ENTRY=Should have received only one entry when scanning for number of rows in metrics table
FUNCTION_IMPLEMENTATION_ERROR_ROW_ID_ORDINAL_NOT_FOUND=Row ID ordinal not found
FUNCTION_IMPLEMENTATION_ERROR_OBJECT_NOT_A_BLOCK=Object is not a Block, but %s
FUNCTION_IMPLEMENTATION_ERROR_OBJECT_NOT_LONG=Object is not a Long, but %s
FUNCTION_IMPLEMENTATION_ERROR_OBJECT_NOT_LONG_OR_INTEGER=Object is not a Long or Integer, but %s
FUNCTION_IMPLEMENTATION_ERROR_OBJECT_NOT_BOOLEAN=Object is not a Boolean, but %s
FUNCTION_IMPLEMENTATION_ERROR_OBJECT_NOT_CALENDER_DATE_OR_LONG=Object is not a Calendar, Date, or Long, but %s
FUNCTION_IMPLEMENTATION_ERROR_OBJECT_NOT_DOUBLE=Object is not a Double, but %s
FUNCTION_IMPLEMENTATION_ERROR_OBJECT_NOT_FLOAT=Object is not a Float, but %s
FUNCTION_IMPLEMENTATION_ERROR_OBJECT_NOT_SHORT=Object is not a Short, but %s
FUNCTION_IMPLEMENTATION_ERROR_OBJECT_NOT_LONG_OR_TIME=Object is not a Long or Time, but %s
FUNCTION_IMPLEMENTATION_ERROR_OBJECT_NOT_LONG_OR_TIMESTAMP=Object is not a Long or Timestamp, but %s
FUNCTION_IMPLEMENTATION_ERROR_OBJECT_NOT_BYTE=Object is not a Byte, but %s
FUNCTION_IMPLEMENTATION_ERROR_OBJECT_NOT_SLICE_BYTE=Object is not a Slice byte[], but %s
FUNCTION_IMPLEMENTATION_ERROR_OBJECT_NOT_SLICE_OR_STRING=Object is not a Slice or String, but %s
FUNCTION_IMPLEMENTATION_ERROR_UNEXPECTED_GROUP_ENUM_TYPE=Unexpected group enum type %s
FUNCTION_IMPLEMENTATION_ERROR_EXPECTED_GROUP_ENUM_TYPE=expected group enum type %s
FUNCTION_IMPLEMENTATION_ERROR_METHOD_DOES_NOT_RETURN_VALID_METHODHANDLE=Method %s does not return valid MethodHandle
FUNCTION_IMPLEMENTATION_ERROR_ALL_PARAMETERS_OF_CONSTRUCTOR_IN_FUNCTION_DEFINITION_ARE_NOT_DEPENDENCIES=All parameters of a constructor in a function definition class must be Dependencies. Signature: %s
ABANDONED_TASK_TASK_HAS_NOT_BEEN_ACCESSED=Task %s has not been accessed since %s: currentTime %s
OPTIMIZER_TIMEOUT_OPTIMIZER_EXHAUSTED_TIME_LIMIT=The optimizer exhausted the time limit of %d ms
OUT_OF_SPILL_SPACE_NO_SPILL_PATHS=No spill paths configured
OUT_OF_SPILL_SPACE_NO_FREE_SPACE_AVAILABLE=No free space available for spill
OUT_OF_SPILL_SPACE_CANNOT_DETERMINE_FREE_SPACE=Cannot determine free space for spill
CONFIGURATION_INVALID_FOR_REFRESH_PERIOD_INVALID=Invalid duration value '%s' for property '%s' in '%s'
CONFIGURATION_INVALID_ERROR_IN_PASSWORD_FILE=Error in password file line %s: %s
CONFIGURATION_INVALID_NO_ROOT_GROUPS_CONFIGURED=No root groups are configured
CONFIGURATION_INVALID_NO_SELECTORS_CONFIGURED=No selectors are configured
CONFIGURATION_INVALID_FAILED_TO_LOAD_ROUTER_CONFIG=Failed to load router config
CONFIGURATION_UNAVAILABLE_FAILED_TO_READ_PASSWORD_FILE=Failed to read password file: %s
CONFIGURATION_UNAVAILABLE_CANNOT_FETCH_RESOURCE_GROUP_CONFIGURATION=Resource group configuration cannot be fetched from source.Current resource group configuration is loaded %s ago
INVALID_RESOURCE_GROUP_CANNOT_ADD_QUERIES=Cannot add queries to %s. It is not a leaf group.
GENERIC_RECOVERY_ERROR_WHILE_RECOVERING_TASK=Encountered error when trying to recover task %s
INDEX_LOADER_TIMEOUT_TIME_TIME_LIMIT=Exceeded the time limit of %s loading indexes for index join
EXCEEDED_TASK_UPDATE_SIZE_LIMIT_TASKUPDATE_SIZE_EXCEEDED_LIMIT=TaskUpdate size of %d Bytes has exceeded the limit of %d Bytes
NODE_SELECTION_NOT_SUPPORTED_NODE_SELECTION_STRATEGY=Unsupported node selection strategy %s
NODE_SELECTION_NOT_SUPPORTED_NODE_SELECTION_STRATEGY_FOR_TTL=Unsupported node selection strategy for TTL scheduling: %s
SPOOLING_STORAGE_ERROR_FAILED_TO_READ_FILE=Failed to read file from TempStorage
SERIALIZED_PAGE_CHECKSUM_ERROR_RECEIVED_CORRUPTED_PAGE=Received corrupted serialized page from host %s
RETRY_QUERY_NOT_FOUND_FAILED_TO_FIND_QUERY=failed to find the query to retry with ID %s
DISTRIBUTED_TRACING_ERROR_DUPLICATED_BLOCK_INSERTED=Duplicated block inserted: %s
DISTRIBUTED_TRACING_ERROR_ADDING_POINT_TO_NON-EXISTING_BLOCK=Adding point to non-existing block: %s
DISTRIBUTED_TRACING_ERROR_TRYING_TO_END_NON-EXISTING_BLOCK=Trying to end a non-existing block: %s
DISTRIBUTED_TRACING_ERROR_SAMPLE_BASED_TRACING_MODE_NOT_SUPPORTED=SAMPLE_BASED Tracing Mode is currently not supported.
GENERIC_SPILL_FAILURE_SPILLING_FAILED=Spilling failed: %s"
GENERIC_SPILL_FAILURE_FAILED_TO_CREATE_SPILL_FILE=Failed to create spill file: %s
GENERIC_SPILL_FAILURE_FAILED_TO_SPILL_PAGES=Failed to spill pages: %s
GENERIC_SPILL_FAILURE_FAILED_TO_READ_SPILLED_PAGES=Failed to read spilled pages: %s
GENERIC_SPILL_FAILURE_FAILED_TO_CLOSE=Failed to close spiller: %s
GENERIC_SPILL_FAILURE_FAILED_TO_SPILL=Failed to spill pages
GENERIC_SPILL_FAILURE_FAILED_TO_READ=Failed to read spilled pages
GENERIC_SPILL_FAILURE_FAILED_TO_DELETE=Failed to delete spill file
INVALID_PLAN_ERROR_MERGE_JOIN_PLAN_NOT_VALID=When grouped execution can not be enabled, merge join plan is not valid. %s is currently set to %s; left node grouped execution capable is %s and  right node grouped execution capable is %s.
INVALID_RETRY_EXECUTION_STRATEGY_NOT_SUPPORTED=Retry execution strategy not supported: %s
PLAN_SERIALIZATION_ERROR_CANNOT_SERIALIZE_TO_JSON=Cannot serialize plan to JSON
QUERY_PLANNING_TIMEOUT_QUERY_OPTIMIZER_EXCEEDED_TIMEOUT=The query optimizer exceeded the timeout of %s.
QUERY_PLANNING_TIMEOUT_QUERY_PLANNER_EXCEEDED_TIMEOUT=The query planner exceeded the timeout of %s.
UNSUPPORTED_ANALYZER_TYPE=Unsupported analyzer type: %s
GENERIC_INSUFFICIENT_RESOURCES_INSUFFICIENT_ACTIVE_WORKER_NODES=Insufficient active worker nodes. Waited %s for at least %s workers, but only %s workers are active
GENERIC_INSUFFICIENT_RESOURCES_INSUFFICIENT_ACTIVE_COORDINATOR_NODE=Insufficient active coordinator nodes. Waited %s for at least %s coordinators, but only %s coordinators are active
GENERIC_INSUFFICIENT_RESOURCES_TABLE_CAPACITY_EXCEEDS_MAX_VALUE=Size of hash table cannot exceed 1 billion entries
GENERIC_INSUFFICIENT_RESOURCES_TABLE_SIZE_EXCEEDS_BUCKET_COUNT=Size of hash table cannot exceed %d entries ( %d)
EXCEEDED_GLOBAL_MEMORY_LIMIT_QUERY_EXCEEDED_USER_LIMIT=Query exceeded distributed user memory limit of %s
EXCEEDED_GLOBAL_MEMORY_LIMIT_QUERY_EXCEEDED_TOTAL_LIMIT=Query exceeded distributed total memory limit of %s defined at the %s
EXCEEDED_GLOBAL_MEMORY_LIMIT_CONTROL_QUERY_USES_MORE_MEMORY=Control query uses more memory than the test cluster memory limit
QUERY_QUEUE_FULL_TOO_MANY_QUEUED_QUERIES=Too many queued queries for "%s"
EXCEEDED_TIME_LIMIT_QUERY_EXCEEDED_MAXIMUM_EXECUTION_TIME_LIMIT=Query exceeded the maximum execution time limit of %s defined at the %s level
EXCEEDED_TIME_LIMIT_QUERY_EXCEEDED_MAXIMUM_TIME_LIMIT=Query exceeded maximum time limit of %s
CLUSTER_OUT_OF_MEMORY_QUERY_KILLED=The cluster is out of memory and %s=true, so this query was killed. It was using %s of memory
CLUSTER_OUT_OF_MEMORY_=Query killed because the cluster is out of memory. Please try again in a few minutes.
EXCEEDED_CPU_LIMIT=Exceeded CPU limit of %s defined at the %s level
EXCEEDED_LOCAL_MEMORY_LIMIT_QUERY_EXCEEDED_PER-NODE_USER_MEMORY_LIMIT=Query exceeded per-node user memory limit of %s [%s]
EXCEEDED_LOCAL_MEMORY_LIMIT_QUERY_EXCEEDED_PER-NODE_TOTAL_MEMORY_LIMIT=Query exceeded per-node total memory limit of %s [%s]
ADMINISTRATIVELY_PREEMPTED_NO_MESSAGE=Query preempted. %s No message provided.
ADMINISTRATIVELY_PREEMPTED_WITH_MESSAGE=Query preempted. Message: %s
EXCEEDED_SCAN_RAW_BYTES_READ_LIMIT_QUERY_EXCEEDED_LIMIT=Query has exceeded Scan Raw Bytes Read Limit of %s
EXCEEDED_OUTPUT_SIZE_LIMIT_QUERY_EXCEEDED_LIMIT=Query has exceeded output size Limit of %s
EXCEEDED_REVOCABLE_MEMORY_LIMIT_QUERY_EXCEEDED_LIMIT=Query exceeded per-node revocable memory limit of %s [%s]
EXCEEDED_LOCAL_BROADCAST_JOIN_MEMORY_LIMIT_QUERY_EXCEEDED_LIMIT=Query exceeded per-node broadcast memory limit of %s [%s]
EXCEEDED_OUTPUT_POSITIONS_LIMIT_QUERY_EXCEEDED_OUTPUT_ROWS_LIMIT=Query has exceeded output rows Limit of %d
NATIVE_EXECUTION_BINARY_NOT_EXIST_FILE_DOES_NOT_EXIST=File does not exist %s
NATIVE_EXECUTION_PROCESS_LAUNCH_ERROR_CANNOT_START_NATIVE_PROCESS=Cannot start native process: %s
NATIVE_EXECUTION_PROCESS_LAUNCH_ERROR_CANNOT_START=Cannot start %s
MISSING_RESOURCE_GROUP_SELECTOR_QUERY_DID_NOT_MATCH=Query did not match any selection rule
UNEXPECTED_ACCUMULO_ERROR_FAILED_TO_GET_SPLITS=Failed to get splits from Accumulo
UNEXPECTED_ACCUMULO_ERROR_FAILED_TO_GET_CONNECTOR=Failed to get connector to Accumulo
UNEXPECTED_ACCUMULO_ERROR_FAILED_TO_CHECK_EXISTENCE=Failed to check for existence or create Accumulo namespace
UNEXPECTED_ACCUMULO_ERROR_FAILED_TO_CREATE_TABLE=Failed to create Accumulo table
UNEXPECTED_ACCUMULO_ERROR_FAILED_TO_SET_LOCALITY_GROUPS=Failed to set locality groups
UNEXPECTED_ACCUMULO_ERROR_FAILED_TO_SET_ITERATOR=Failed to set iterator on table
UNEXPECTED_ACCUMULO_ERROR_FAILED_TO_DELETE_TABLE=Failed to delete Accumulo table
UNEXPECTED_ACCUMULO_ERROR_FAILED_TO_RENAME_TABLE=Failed to rename table
UNEXPECTED_ACCUMULO_ERROR_WHILE_GETTING_CARDINALITY=Exception when getting cardinality
UNEXPECTED_ACCUMULO_ERROR_INDEX_MUTATION_REJECTED=Index mutation rejected by server
UNEXPECTED_ACCUMULO_ERROR_INDEX_MUTATION_REJECTED_ON_FLUSH=Index mutation was rejected by server on flush
UNEXPECTED_ACCUMULO_ERROR_MUTATION_REJECTED_ON_CLOSE=Mutation was rejected by server on close
UNEXPECTED_ACCUMULO_ERROR_WHILE_GETTING_INDEX_RANGES=Exception when getting index ranges
UNEXPECTED_ACCUMULO_ERROR_CREATE_BATCHWRITER_INDEXER=Accumulo error when creating BatchWriter and/or Indexer
UNEXPECTED_ACCUMULO_ERROR_MUTATION_REJECTED=Mutation rejected by server
UNEXPECTED_ACCUMULO_ERROR_MUTATION_REJECTED_ON_FLUSH=Mutation rejected by server on flush
UNEXPECTED_ACCUMULO_ERROR_FAILED_TO_CREATE_BATCH_SCANNER=Failed to create batch scanner for table %s
ZOOKEEPER_ERROR_METADATA_ROOT_DOES_NOT_EXIST=ZK error checking metadata root
ZOOKEEPER_ERROR_2_CHECKING/CREATING_DEFAULT_SCHEMA=ZK error checking/creating default schema
ZOOKEEPER_ERROR_FETCHING_SCHEMA=Error fetching schemas
ZOOKEEPER_ERROR_WHILE_CHECKING_SCHEMA_EXISTS=Error checking if schema exists
ZOOKEEPER_ERROR_FETCHING_TABLE=Error fetching table
ZOOKEEPER_ERROR_FETCHING_VIEW=Error fetching view
ZOOKEEPER_ERROR_CHECKING_TABLE_ALREADY_EXISTS=ZK error when checking if table already exists
ZOOKEEPER_ERROR_CREATE_TABLE_ZNODE=Error creating table znode in ZooKeeper
ZOOKEEPER_ERROR_DELETE_TABLE_METADATA=ZK error when deleting table metadata
ZOOKEEPER_ERROR_CHECK_VIEW_ALREADY_EXISTS=ZK error when checking if view already exists
ZOOKEEPER_ERROR_CREATE_VIEW_ZNODE=Error creating view znode in ZooKeeper
ZOOKEEPER_ERROR_DELETE_VIEW=ZK error when deleting view metadata
ZOOKEEPER_ERROR_CHECK_IF_PATH_IS_ACCUMULO_TABLE_OBJECT=Error checking if path %s is an AccumuloTable object
ZOOKEEPER_ERROR_CHECK_PATH_IS_ACCUMULO_TABLE_OBJECT=Error checking if path is an AccumuloView object
IO_ERROR_FROM_SERIALIZER_ON_READ=Caught IO error from serializer on read
ACCUMULO_TABLE_DNE_TABLE_DOES_NOT_EXIST=Table %s does not exist
ACCUMULO_TABLE_DNE_FAILED_TO_SET_LOCALITY_GROUP=Failed to set locality groups, table does not exist
ACCUMULO_TABLE_DNE_FAILED_TO_SET_ITERATOR=Failed to set iterator, table does not exist
ACCUMULO_TABLE_DNE_FAILED_TO_DELETE_ACCUMULO_TABLE=Failed to delete Accumulo table, does not exist
ACCUMULO_TABLE_DNE_FAILED_TO_RENAME_TABLE=Failed to rename table, old table does not exist
ACCUMULO_TABLE_DNE_ACCUMULO_TABLE_DOES_NOT_EXIST=Accumulo table does not exist
ACCUMULO_TABLE_DNE_ERROR_WHILE_CREATING_BATCHWRITER_INDEXER=Accumulo error when creating BatchWriter and/or Indexer, table does not exist
ACCUMULO_TABLE_EXISTS_CANNOT_CREATE_INTERNAL_TABLE=Cannot create internal table when an Accumulo table already exists
ACCUMULO_TABLE_EXISTS_INDEX_TABLE_INDEX_METRICS_ALREADY_EXISTS=Internal table is indexed, but the index table and/or index metrics table(s) already exist
ACCUMULO_TABLE_EXISTS_TABLE_ALREADY_EXISTS=Table %s already exists
ACCUMULO_TABLE_EXISTS_ALREADY_EXISTS=Accumulo table already exists
ACCUMULO_TABLE_EXISTS_FAILED_TO_RENAME_TABLE=Failed to rename table, new table already exists
MINI_ACCUMULO_FAILED_SHUT_DOWN_MAC=Failed to shut down MAC instance
MINI_ACCUMULO_FAILED_CLEAN_UP_MAC=Failed to clean up MAC directory
ATOP_CANNOT_START_PROCESS_ERROR_CANNOT_START=Cannot start %s
ATOP_READ_TIMEOUT_READ_FROM_ATOP_PROCESS=Timeout reading from atop process
BIGQUERY_VIEW_DESTINATION_TABLE_CREATION_FAILED_ERROR_CREATING_DESTINATION_TABLE=Error creating destination table
BIGQUERY_FAILED_TO_EXECUTE_QUERY_COMPUTE_EMPTY_PROJECTION=Failed to compute empty projection
BIGQUERY_QUERY_FAILED_UNKNOWN_RUN_QUERY=Failed to run the query [%s]
BIGQUERY_UNSUPPORTED_TYPE_FOR_SLICE_UNHANDLED_TYPE=Unhandled type for Slice: %s
BIGQUERY_UNSUPPORTED_COLUMN_TYPE_UNHANDLED_TYPE=Unhandled type for %s: %s
BIGQUERY_UNSUPPORTED_COLUMN_TYPE_UNSUPPORTED_TYPE_CONVERSION=Not support type conversion for BigQuery data type: %s
BIGQUERY_UNSUPPORTED_TYPE_FOR_BLOCK_UNHANDLED_TYPE=Unhandled type for Block: %s
BIGQUERY_UNSUPPORTED_TYPE_FOR_LONG_UNHANDLED_TYPE=Unhandled type for %s: %s
BIGQUERY_UNSUPPORTED_TYPE_FOR_VARBINARY_UNHANDLED_TYPE=Unhandled type for VarBinaryType: %s
BIGQUERY_TABLE_DISAPPEAR_DURING_LIST_TABLE_NOT_FOUND=Table disappeared during listing operation
BIGQUERY_ERROR_END_OF_AVRO_BUFFER_ERROR_DETERMINING_END=Error determining the end of Avro buffer
BIGQUERY_ERROR_READING_NEXT_AVRO_RECORD_NULL=Error reading next Avro Record
CASSANDRA_METADATA_ERROR_CLUSTER_METADATA_NOT_AVAILABLE=The cluster metadata is not available. Please make sure that the Cassandra cluster is up and running, and that the contact points are specified correctly.
CASSANDRA_VERSION_ERROR_CLUSTER_VERSION_NOT_AVAILABLE=The cluster version is not available. Please make sure that the Cassandra cluster is up and running, and that the contact points are specified correctly.
JDBC_ERROR_NEW_COLUMN_NAME=Query: %s
JDBC_ERROR_FAILED_TO_FIND_REMOTE_TABLE_NAME=Failed to find remote table name: %s
JDBC_ERROR_FAILED_TO_FIND_REMOTE_SCHEMA_NAME=Failed to find remote schema name: %s
JDBC_ERROR_TYPE_NAME_MISSING=Type name is missing: %s
CLICKHOUSE_PUSHDOWN_UNSUPPORTED_EXPRESSION_LOGICAL_BINARY_OPS_FILTER_NOT_SUPPORTED=%s is not supported in ClickHouse filter
CLICKHOUSE_PUSHDOWN_UNSUPPORTED_EXPRESSION_UNKNOWN_LOGICAL_BINARY=Unknown logical binary: %s
CLICKHOUSE_PUSHDOWN_UNSUPPORTED_EXPRESSION_BETWEEN_OPERATOR_NOT_SUPPORTED=Between operator not supported: %s
CLICKHOUSE_PUSHDOWN_UNSUPPORTED_EXPRESSION_NOT_OPERATOR_NOT_SUPPORTED_WITHOUT_IN=NOT operator is supported only on top of IN operator. Received: %s
CLICKHOUSE_PUSHDOWN_UNSUPPORTED_EXPRESSION_NON-IMPLICIT_CAST_NOT_SUPPORTED=Non implicit casts not supported: %s
CLICKHOUSE_PUSHDOWN_UNSUPPORTED_EXPRESSION_CAST_OPERATOR_TYPE_NOT_SUPPORTED=This type of CAST operator not supported: %s
CLICKHOUSE_PUSHDOWN_UNSUPPORTED_EXPRESSION_ARITHMETIC_EXPRESSION_NOT_SUPPORTED=Arithmetic expressions are not supported in ClickHouse filter: %s
CLICKHOUSE_PUSHDOWN_UNSUPPORTED_EXPRESSION_FUNCTION_NOT_SUPPORTED=Function %s not supported in ClickHouse filter
CLICKHOUSE_PUSHDOWN_UNSUPPORTED_EXPRESSION_STRUCT_DEREFERENCE_NOT_SUPPORTED=ClickHouse does not support struct dereference: %s
CLICKHOUSE_PUSHDOWN_UNSUPPORTED_EXPRESSION_LAMBDA_NOT_SUPPORTED=ClickHouse does not support lambda: %s
CLICKHOUSE_PUSHDOWN_UNSUPPORTED_EXPRESSION_SPECIAL_FORM_NOT_SUPPORTED=ClickHouse does not support special form: %s
CLICKHOUSE_PUSHDOWN_UNSUPPORTED_EXPRESSION_UNSUPPORTED_FUNCTION=Unsupported function in ClickHouse aggregation: %s
CLICKHOUSE_PUSHDOWN_UNSUPPORTED_EXPRESSION_INPUT_REFERENCE_NOT_SUPPORTED=Input reference not supported: %s
CLICKHOUSE_PUSHDOWN_UNSUPPORTED_EXPRESSION_CALL_NOT_SUPPORTED=Call not supported: %s
CLICKHOUSE_PUSHDOWN_UNSUPPORTED_EXPRESSION_CONSTANT_NOT_SUPPORTED=Constant not supported: %s
CLICKHOUSE_PUSHDOWN_UNSUPPORTED_EXPRESSION_UNSUPPORTED_AGGREGATION_NODE=Unsupported aggregation node %s
CLICKHOUSE_PUSHDOWN_UNSUPPORTED_EXPRESSION_NULL_CONSTANT_EXPRESSION=Null constant expression: %s with value of type: %s
CLICKHOUSE_PUSHDOWN_UNSUPPORTED_EXPRESSION_CANNOT_HANDLE_CONSTANT_EXPRESSION=Cannot handle the constant expression: %s with value of type: %s
CLICKHOUSE_PUSHDOWN_UNSUPPORTED_EXPRESSION_UNSUPPPORTED_PUSHDOWN_FOR_CONNECTOR_PLAN_NODE=Unsupported pushdown for ClickHouse connector with plan node of type %s
CLICKHOUSE_PUSHDOWN_UNSUPPORTED_EXPRESSION_UNSUPPPORTED_PUSHDOWN_FOR_CONNECTOR=Unsupported pushdown for ClickHouse connector. Expect variable reference, but get: %s
CLICKHOUSE_PUSHDOWN_UNSUPPORTED_EXPRESSION_UNSUPPPORTED_PUSHDOWN_FOR_CONNECTOR_UNKNOWN_EXPRESSION=Unsupported pushdown for clickhouse connector. Unknown aggregation expression: %s
CLICKHOUSE_PUSHDOWN_UNSUPPORTED_EXPRESSION_UNSUPPPORTED_PUSHDOWN_FOR_CONNECTOR_UNSUPPORTED_FUNCTION=Unsupported pushdown for ClickHouse connector. Aggregation function: %s not supported
CLICKHOUSE_PUSHDOWN_UNSUPPORTED_EXPRESSION_FILTER_ON_TOP_OF_AGGREGATION_NODE=ClickHouse does not support filter on top of AggregationNode.
CLICKHOUSE_QUERY_GENERATOR_FAILURE_EXPECTED_TABLE_SCAN_NODE_ID=Expected to find a clickhouse table scan node id
CLICKHOUSE_QUERY_GENERATOR_FAILURE_EXPECTED_TABLE_SCAN_NODE=Expected to find a clickhouse table scan node
CLICKHOUSE_QUERY_GENERATOR_FAILURE_EXPECTED_TABLE_HANDLE=Expected to find a clickhouse table handle
CLICKHOUSE_QUERY_GENERATOR_FAILURE_LIMIT_OUT_OF_BOUNDS=Invalid limit: %d
CLICKHOUSE_QUERY_GENERATOR_FAILURE_COULD_NOT_PUSHDOWN_MULTIPLE_AGGREGATION=Could not pushdown multiple aggregates in the presence of group by and limit
CLICKHOUSE_QUERY_GENERATOR_FAILURE_EMPTY_QUERY=Empty ClickHouse query
CLICKHOUSE_QUERY_GENERATOR_FAILURE_TABLE_NAME_MISSING=Table name missing in ClickHouse query
DECODER_CONVERSION_NOT_SUPPORTED_TO_BOOLEAN=conversion to boolean not supported
DECODER_CONVERSION_NOT_SUPPORTED_TO_LONG=conversion to long not supported
DECODER_CONVERSION_NOT_SUPPORTED_TO_DOUBLE=conversion to double not supported
DECODER_CONVERSION_NOT_SUPPORTED_TO_SLICE=conversion to Slice not supported
DECODER_CONVERSION_NOT_SUPPORTED_TO_BLOCK=conversion to Block not supported
DECODER_CONVERSION_NOT_SUPPORTED_CANNOT_DECODE_OBJECT=cannot decode object of '%s' as '%s' for column '%s'
DECODER_CONVERSION_NOT_SUPPORTED_CANNOT_PARSE_VALUE=could not parse value '%s' as '%s' for column '%s'
DECODER_CONVERSION_NOT_SUPPORTED_CANNOT_PARSE_NON-VALUE=could not parse non-value node as '%s' for column '%s'
DECODER_CONVERSION_NOT_SUPPORTED_START_OFFSET_GREATER_THAN_LENGTH=start offset %s for column '%s' must be less that or equal to value length %s
DECODER_CONVERSION_NOT_SUPPORTED_END_OFFSET_GREATER_THAN_LENGTH=end offset %s for column '%s' must be less that or equal to value length %s
DECODER_CONVERSION_NOT_SUPPORTED_TO_BOOLEAN_FIELDTYPE=conversion '%s' to boolean not supported
DECODER_CONVERSION_NOT_SUPPORTED_TO_LONG_FIELDTYPE=conversion '%s' to long not supported
DECODER_CONVERSION_NOT_SUPPORTED_TO_DOUBLE_FIELDTYPE=conversion '%s' to double not supported
DELTA_UNSUPPORTED_COLUMN_TYPE_UNSUPPORTED_DATATYPE_FOR_PARTITION_COLUMN=Unsupported data type '%s' for partition column %s
DELTA_UNSUPPORTED_COLUMN_TYPE_COLUMN_CONTAINS_UNSUPPORTED_DATATYPE=Column '%s' in Delta table %s contains unsupported data type: %s
DELTA_UNSUPPORTED_DATA_FORMAT_HAS_UNSUPPORTED_DATA_FORMAT=Delta table %s has unsupported data format: %s. Currently only Parquet data format is supported
DELTA_PARQUET_SCHEMA_MISMATCH_COLUMN_TYPE_DECLARED_MISMATCH=The column %s of table %s is declared as type %s, but the Parquet file (%s) declares the column as type %s
DELTA_CANNOT_OPEN_SPLIT_OPENING_HIVE_SPLIT=Error opening Hive split %s (offset=%s, length=%s): %s
DELTA_MISSING_DATA_ERROR_OPENING_HIVE_SPLIT=Error opening Hive split %s (offset=%s, length=%s): %s
DELTA_INVALID_PARTITION_VALUE_CANNOT_PARSE_PARTITION_VALUE_IN_FILE=Can not parse partition value '%s' of type '%s' for partition column '%s' in file '%s'
DELTA_INVALID_PARTITION_VALUE_CANNOT_PARSE_PARTITION_VALUE=Can not parse partition value '%s' of type '%s' for partition column '%s'
DRUID_METADATA_ERROR_MALFORMED_LOAD_SPECIFICATION=Malformed segment loadSpecification: %s
DRUID_METADATA_ERROR_UNSUPPORTED_FILESYSTEM=Unsupported segment filesystem: %s
DRUID_DEEP_STORAGE_ERROR_FAILED_TO_CREATE_PAGE_SOURCE=Failed to create page source on %s
DRUID_DEEP_STORAGE_ERROR_INGESTION_FAILED=Ingestion failed on %s
DRUID_DEEP_STORAGE_ERROR_READ_FROM_POSITION=Error reading from %s at position %s
DRUID_SEGMENT_LOAD_ERROR_FAILED-TO_LOAD_DRUID_SEGMENT=failed to load druid segment
DRUID_SEGMENT_LOAD_ERROR_FILE_DOES_NOT_EXIST=Internal file %s does not exist
DRUID_SEGMENT_LOAD_ERROR_MALFORMED_METADATA_FILE_GOT_NULL=Malformed metadata file: first line should be version,maxChunkSize,numChunks, got null.
DRUID_SEGMENT_LOAD_ERROR_MALFORMED_METADATA_FILE_UNKNOWN_VERSION=Malformed metadata file: unknown version[%s], v1 is all I know.
DRUID_SEGMENT_LOAD_ERROR_MALFORMED_METADATA_FILE_WRONG_NUMBER_OF_SPLITS=Malformed metadata file: wrong number of splits[%d] in line[%s]
DRUID_SEGMENT_LOAD_ERROR_FILE_NOT_FOUND_IN_ZIP=Zip does not contain file: %s
DRUID_SEGMENT_LOAD_ERROR_MALFORMED_FILE=Malformed zip file: %s
DRUID_SEGMENT_LOAD_ERROR_ZIP_FILE_MALFORMED=Zip file '%s' is malformed. It does not contain an end of central directory record.
DRUID_SEGMENT_LOAD_ERROR_MALFORMED_FILE_HEADER=Malformed Central Directory File Header; does not start with %08x
DRUID_SEGMENT_LOAD_ERROR_MALFORMED_RECORD=Malformed End of Central Directory Record; does not start with %08x
DRUID_SEGMENT_LOAD_ERROR_FILE_COMMENT_TOO_LONG=File comment too long. Is %d; max %d.
DRUID_SEGMENT_LOAD_ERROR_FILE_HEADER_NOT_FOUND=The file '%s' is not a correctly formatted zip file: Expected a File Header at offset %d, but not present.
DRUID_UNSUPPORTED_TYPE_ERROR=Unsupported type: %s
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_UNSUPPORTED_OPERATOR=Unsupported operator: %s to pushdown for Druid connector.
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_UNSUPPORTED_TIME_FUNCTION=Unsupported time function: %s to pushdown for Druid connector.
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_UNSUPPORTED_INTERVAL_UNIT=Unsupported interval unit: %s to pushdown for Druid connector.
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_UNSUPPORTED_BINARY_EXPRESSION=Unsupported binary expression: %s to pushdown for Druid connector.
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_UNSUPPORTED_ARITHMETIC_EXPRESSION=Unsupported arithmetic expression: %s to pushdown for Druid connector.
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_UNSUPPORTED_FUNCTION=Unsupported function: %s to pushdown for Druid connector.
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_EXPECTED_STRING_LITERAL=Expected string literal but found: %s to pushdown for Druid connector.
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_COULD_NOT_DIG_FUNCTION_OUT_OF_EXPRESSION=Could not dig function out of expression: %s, inside of: %s to pushdown for Druid connector.
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_OPERATOR_NOT_SUPPORTED=%s is not supported in Druid filter
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_UNKNOWN_LOGICAL_BINARY=Unknown logical binary: %s
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_BETWEEN_OPERATOR_NOT_SUPPORTED=Between operator not supported: %s
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_NOT_OPERATOR_NOT_SUPPORTED_WITHOUT_IN=NOT operator is supported only on top of IN operator. Received: %s
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_NON-IMPLICIT_CAST_NOT_SUPPORTED=Non implicit casts not supported: %s
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_CAST_OPERATOR_NOT_SUPPORTED=This type of CAST operator not supported: %s
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_ARITHMETIC_EXPRESSION_NOT_SUPPORTED=Arithmetic expressions are not supported in Druid filter: %s
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_FUNCTION_CALL_NOT_SUPPORTED=Function %s not supported in Druid filter
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_STRUCT_DEREFERENCE_NOT_SUPPORTED=Druid does not support struct dereference: %s
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_LAMBDA_NOT_SUPPORTED=Druid does not support lambda: %s
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_SPECIAL_FORM_NOT_SUPPORTED=Druid does not support special form: %s
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_UNSUPPORTED_FUNCTION_IN_AGGREGATION=Unsupported function in Druid aggregation: %s
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_INPUT_REFERENCE_NOT_SUPPORTED=Input reference not supported: %s
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_CALL_NOT_SUPPORTED=Call not supported: %s
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_CONSTANT_NOT_SUPPORTED=Constant not supported: %s
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_UNSUPPORTED_AGGREGATION_NODE=Unsupported aggregation node %s
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_UNSUPPORTED_AGGREGATION_NODE_WITH_MASK=Unsupported aggregation node with mask %s
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_NULL_CONSTANT_EXPRESSION=Null constant expression: %s with value of type: %s
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_CANNOT_HANDLE_CONSTANT_EXPRESSION=Cannot handle the constant expression: %s with value of type: %s
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_UNSUPPORTED_PUSHDOWN_WITH_PLAN_NODE=Unsupported pushdown for Druid connector with plan node of type %s
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_UNSUPPORTED_PUSHDOWN=Unsupported pushdown for Druid connector. Expect variable reference, but get: %s
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_UNSUPPORTED_PUSHDOWN_UNKNOWN_AGGREGATION_EXPRESSION=Unsupported pushdown for Druid connector. Unknown aggregation expression: %s
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_UNSUPPORTED_PUSHDOWN_UNSUPPORTED_AGGREGATION_FUNCTION=Unsupported pushdown for Druid connector. Aggregation function: %s not supported
DRUID_PUSHDOWN_UNSUPPORTED_EXPRESSION_UNSUPPORTED_FILTER_ON_AGGREGATION_NODE=Druid does not support filter on top of AggregationNode.
DRUID_QUERY_GENERATOR_FAILURE_EXPECTED_TABLE_SCAN_NODE_ID=Expected to find a druid table scan node id
DRUID_QUERY_GENERATOR_FAILURE_EXPECTED_TABLE_SCAN_NODE=Expected to find a druid table scan node
DRUID_QUERY_GENERATOR_FAILURE_EXPECTED_TABLE_HANDLE=Expected to find a druid table handle
DRUID_QUERY_GENERATOR_FAILURE_OUTSIDE_LIMIT=Invalid limit: %d
DRUID_QUERY_GENERATOR_FAILURE_COULD_NOT_PUSHDOWN_MULTIPLE_AGGREGATIONS=Could not pushdown multiple aggregates in the presence of group by and limit
DRUID_QUERY_GENERATOR_FAILURE_QUERY_EMPTY=Empty Druid query
DRUID_QUERY_GENERATOR_FAILURE_TABLE_NAME_MISSING=Table name missing in Druid query
DRUID_BROKER_RESULT_ERROR_CLIENT_RESPONSE_ERROR=Parse druid client response error
DRUID_BROKER_RESULT_ERROR_REQUEST_FAILED=Request to worker failed
DRUID_BROKER_RESULT_ERROR_TYPE_NOT_JSON=Response received was not of type %s
DRUID_BROKER_RESULT_ERROR_UNABLE_TO_READ_RESPONSE=Unable to read response from worker
DRUID_AMBIGUOUS_OBJECT_NAME_FOUND_AMBIGUOUS_NAME=Found ambiguous names in Druid when looking up '%s':
ELASTICSEARCH_TYPE_MISMATCH_TYPE_ARRAY=Expected list of elements for field '%s' of type ARRAY: %s [%s]
ELASTICSEARCH_TYPE_MISMATCH_TYPE_BIGINT=Expected a numeric value for field %s of type BIGINT: %s [%s]
ELASTICSEARCH_TYPE_MISMATCH_TYPE_BOOLEAN=Expected a boolean value for field %s of type BOOLEAN: %s [%s]
ELASTICSEARCH_TYPE_MISMATCH_TYPE_DOUBLE=Expected a numeric value for field %s of type DOUBLE: %s [%s]
ELASTICSEARCH_TYPE_MISMATCH_TYPE_INTEGER=Expected a numeric value for field '%s' of type INTEGER: %s [%s]
ELASTICSEARCH_TYPE_MISMATCH_TYPE_IP=Expected a string value for field '%s' of type IP: %s [%s]
ELASTICSEARCH_TYPE_MISMATCH_TYPE_REAL=Expected a numeric value for field %s of type REAL: %s [%s]
ELASTICSEARCH_TYPE_MISMATCH_TYPE_ROW=Expected object for field '%s' of type ROW: %s [%s]
ELASTICSEARCH_TYPE_MISMATCH_TYPE_SMALLINT=Expected a numeric value for field '%s' of type SMALLINT: %s [%s]
ELASTICSEARCH_TYPE_MISMATCH_TYPE_SMALLINT_VALUE_OUT_OF_RANGE=Value out of range for field '%s' of type SMALLINT: %s
ELASTICSEARCH_TYPE_MISMATCH_NOT_SINGLE_VALUE=Expected single value for column '%s', found: %s
ELASTICSEARCH_TYPE_MISMATCH_TYPE_TINYINT_VALUE_OUT_OF_RANGE=Value out of range for field '%s' of type TINYINT: %s
ELASTICSEARCH_TYPE_MISMATCH_TYPE_TINYINT=Expected a numeric value for field '%s' of type TINYINT: %s [%s]
ELASTICSEARCH_TYPE_MISMATCH_TYPE_VARBINARY=Expected a string value for field '%s' of type VARBINARY: %s [%s]
ELASTICSEARCH_TYPE_MISMATCH_TYPE_VARCHAR=Expected a string or numeric value for field '%s' of type VARCHAR: %s [%s]
HIVE_METASTORE_ERROR_CURRENT_STATISTICS_NULL=currentStatistics is null
HIVE_METASTORE_ERROR_LOCATION_SET=Database can not be created with a location set
HIVE_METASTORE_ERROR_NOT_EMPTY=Database %s is not empty
HIVE_METASTORE_ERROR_COULD_NOT_RENAME=Could not rename database metadata directory
HIVE_METASTORE_ERROR_TABLE_DIRECTORY_NOT_EQUAL_TO_TABLE_METADATA_DIRECTORY=Table directory must be %s
HIVE_METASTORE_ERROR_LOCATION_DOES_NOT_EXIST=External table location does not exist
HIVE_METASTORE_ERROR_LOCATION_INSIDE_SYSTEM_METADATA_DIRECTORY=External table location can not be inside the system metadata directory
HIVE_METASTORE_ERROR_COULD_NOT_VALIDATE_LOCATION=Could not validate external location
HIVE_METASTORE_ERROR_NOT_VIRTUAL_VIEW=Only views can be updated with replaceTable
HIVE_METASTORE_ERROR_TABLE_NAME_MISMATCH=Replacement table must have same name
HIVE_METASTORE_ERROR_CANNOT_RENAME_TABLE_DIRECTORY=Could not rename table directory
HIVE_METASTORE_ERROR_SCHEMAPATH_ALREADY_EXISTS=Partition already exists
HIVE_METASTORE_ERROR_CANNOT_WRITE_PARTITION_SCHEMA=Could not write partition schema
HIVE_METASTORE_ERROR_PARTITION_DIRECTORY_NOT_EQUAL_TO_PARTITION_METADATA_DIRECTORY=Partition directory must be %s
HIVE_METASTORE_ERROR_PARTITION_LOCATION_DOES_NOT_EXIST=External partition location does not exist
HIVE_METASTORE_ERROR_PARTITION_INSIDE_SYSTEM_METADATA_DIRECTORY=External partition location can not be inside the system metadata directory
HIVE_METASTORE_ERROR_COULD_NOT_VALIDATE_PARTITION_LOCATION=Could not validate external partition location
HIVE_METASTORE_ERROR_COULD_NOT_LIST_PARTITION_DIRECTORIES=Error listing partition directories
HIVE_METASTORE_ERROR_PERMISSION_DIRECTORY_DOES_NOT_EXIST=Could not create permissions directory
HIVE_METASTORE_ERROR_CANNOT_DELETE_PERMISSION=Could not delete table permissions
HIVE_METASTORE_ERROR_CANNOT_DELETE_METADATA_DIRECTORY=Could not delete metadata directory
HIVE_METASTORE_ERROR_COULD_NOT_READ_FROM_JSON=Could not read %s
HIVE_METASTORE_ERROR_FILE_ALREADY_EXISTS=%s file already exists
HIVE_METASTORE_ERROR_COULD_NOT_WRITE_JSON=Could not write %s
HIVE_METASTORE_ERROR_COULD_NOT_DELETE_SCHEMA=Could not delete %s schema
HIVE_METASTORE_ERROR_CANNOT_FETCH_PARTITION_FROM_GLUE=Failed to fetch partitions from Glue Data Catalog
HIVE_METASTORE_ERROR_RETURNED_MULTIPLE_PARTITIONS=Metastore returned multiple partitions for name: %s
HIVE_METASTORE_ERROR_=Hive metastore only added %s of %s partitions
HIVE_CURSOR_ERROR_FAILED_TO_READ_FILE=Failed to read ORC file: %s
HIVE_CURSOR_ERROR_FAILED_TO_READ=Failed to read RC file: %s
HIVE_CANNOT_OPEN_SPLIT_ERROR_OPENING_HIVE_SPLIT=Error opening Hive split %s (offset=%s, length=%s) using %s: %s
HIVE_CANNOT_OPEN_SPLIT_ERROR_OPENING_HIVE_SPLIT_DATASOURCE=Error opening Hive split %s (offset=%s, length=%s): %s
HIVE_FILE_NOT_FOUND_LOCATION_DOES_NOT_EXIST=Partition location does not exist: %s
HIVE_UNKNOWN_ERROR_VALUE_NULL=Partition %s does not have a value for partition column %s
HIVE_UNKNOWN_ERROR_UNSUPPORTED_CACHE_QUOTA_SCOPE=%s is not supported
HIVE_BAD_DATA_TEXT_LINE_LENGTH_LIMIT_EXCEEDED=Line too long in text file: %s
HIVE_BAD_DATA_ERROR_PARSING_SYMLINK=Error parsing symlinks from: %s
HIVE_BAD_DATA_EMPTY_FILE_ORC=ORC file is empty: %s
HIVE_BAD_DATA_FOOTER_OFFSET_LESS_THAN_ZERO=Malformed PageFile format, incorrect footer length.
HIVE_BAD_DATA_INVALID_COMPRESSION_METHOD=%s is invalid compression method in the footer of %s
HIVE_BAD_DATA_INCORRECT_STRIPE_COUNT=Malformed PageFile format, incorrect stripe count.
HIVE_BAD_DATA_CORRUPTED_FILE=Corrupted RC file: %s
HIVE_BAD_DATA_EMPTY_FILE_RC=RCFile is empty: %s
HIVE_PARTITION_SCHEMA_MISMATCH_NO_PARTITION_BUCKET_PROPERTY=Hive table (%s) is bucketed but partition (%s) is not bucketed
HIVE_PARTITION_SCHEMA_MISMATCH_BUCKET_COUNT_INCOMPATIBLE=Hive table (%s) bucketing (columns=%s, buckets=%s) is not compatible with partition (%s) bucketing (columns=%s, buckets=%s)
HIVE_PARTITION_SCHEMA_MISMATCH_SCHEMA_MISMATCH_INCOMPATIBLE=There is a mismatch between the table and partition schemas. The types are incompatible and cannot be coerced. The column '%s' in table '%s' is declared as type '%s', but partition '%s' declared column '%s' as type '%s'.
HIVE_PARTITION_SCHEMA_MISMATCH_SCHEMA_MISMATCH=There is a mismatch between the table and partition schemas.  The column '%s' in table '%s.%s' is declared as type '%s', but partition '%s' declared column '%s' as type '%s'.
HIVE_PARTITION_SCHEMA_MISMATCH_WRITE_INTO_EXISTING_PARTITION=You are trying to write into an existing partition in a table. The table schema has changed since the creation of the partition. Inserting rows into such partition is not supported. The column '%s' in table '%s' is declared as type '%s', but partition '%s' declared column '%s' as type '%s'.
HIVE_PARTITION_SCHEMA_MISMATCH_COLUMN_MISMATCH=The column %s of table %s is declared as type %s, but the Parquet file (%s) declares the column as type %s
HIVE_INVALID_PARTITION_VALUE_PARTITION_KEY_NULL=partition key value cannot be null for field: %s
HIVE_INVALID_PARTITION_VALUE_FOR_BOOLEAN_PARTITION_KEY=Invalid partition value '%s' for BOOLEAN partition key: %s
HIVE_INVALID_PARTITION_VALUE_FOR_BIGINT_PARTITION_KEY=Invalid partition value '%s' for BIGINT partition key: %s
HIVE_INVALID_PARTITION_VALUE_FOR_INTEGER_PARTITION_KEY=Invalid partition value '%s' for INTEGER partition key: %s
HIVE_INVALID_PARTITION_VALUE_FOR_SMALLINT_PARTITION_KEY=Invalid partition value '%s' for SMALLINT partition key: %s
HIVE_INVALID_PARTITION_VALUE_FOR_TINYINT_PARTITION_KEY=Invalid partition value '%s' for TINYINT partition key: %s
HIVE_INVALID_PARTITION_VALUE_FOR_FLOAT_PARTITION_KEY=Invalid partition value '%s' for FLOAT partition key: %s
HIVE_INVALID_PARTITION_VALUE_FOR_DOUBLE_PARTITION_KEY=Invalid partition value '%s' for DOUBLE partition key: %s
HIVE_INVALID_PARTITION_VALUE_FOR_DATE_PARTITION_KEY=Invalid partition value '%s' for DATE partition key: %s
HIVE_INVALID_PARTITION_VALUE_FOR_TIMESTAMP_PARTITION_KEY=Invalid partition value '%s' for TIMESTAMP partition key: %s
HIVE_INVALID_PARTITION_VALUE_PARTITION_KEY_TYPE=Invalid partition value '%s' for %s partition key: %s
HIVE_INVALID_PARTITION_VALUE_KEY_VALUE_NULL=partition key value cannot be null for field: %s
HIVE_INVALID_PARTITION_VALUE_INVALID_VALUE_CHARACTER=Hive partition keys can only contain printable ASCII characters (0x20 - 0x7E). Invalid value: %s
HIVE_TIMEZONE_MISMATCH_JVM_TIMEZONE_MISMATCH=To write Hive data, your JVM timezone must match the Hive storage timezone. Add -Duser.timezone=%s to your JVM arguments.
HIVE_INVALID_METADATA_BUCKET_COLUMN_HANDLE_NULL=Table '%s.%s' is bucketed on non-existent column '%s'
HIVE_INVALID_METADATA_NOT_THE_EXPECTED_PARTITION_KEY_VALUE_SIZE=Expected %d partition key values, but got %d
HIVE_INVALID_METADATA_BUCKET_TABLE_SPECIFYING_PREFERRED_ORDERING_COLUMN=bucketed table %s should not specify preferred_ordering_columns
HIVE_INVALID_METADATA_ONLY_SINGLE_CHARACTER_ALLOWED=Only single character can be set for property: %s
HIVE_INVALID_METADATA_DIFFERENT_VALUES_IN_SERDE_AND_TABLE_PROPERTY=Different values for '%s' set in serde properties and table properties: '%s' and '%s'
HIVE_INVALID_METADATA_TABLE_CONTAINS_DUPLICATE_COLUMNS=Hive metadata for table %s is invalid: Table descriptor contains duplicate columns
HIVE_INVALID_METADATA_HAS_NULL_COLUMNS=Table '%s' or partition '%s' has null columns
HIVE_INVALID_METADATA_INVALID_VALUE=Invalid value for %s property: %s
HIVE_INVALID_METADATA_TABLE_DROPPED_DURING_INSERT=Table %s.%s was dropped during insert
HIVE_INVALID_METADATA_COLUMN_DOES_NOT_EXIST=Sorting column '%s' does not exist in table '%s.%s'
HIVE_INVALID_METADATA_MISMATCHED_METADATA=Partition '%s' in table '%s.%s' has mismatched metadata for column names and types
HIVE_INVALID_METADATA_FLAT_MAP_WRITER_NOT_ENABLED=Table '%s' is flattened, but flat map writer is not enabled for this session.
HIVE_INVALID_METADATA_BUCKETCOLS_NOT_SET=Table/partition metadata has 'numBuckets' set, but 'bucketCols' is not set: %s
HIVE_INVALID_METADATA_INVALID_STRUCT_TYPE=Invalid Hive struct type: %s
HIVE_INVALID_METADATA_INVALID_SORTING_ORDER=Table/partition metadata has invalid sorting order: %s
HIVE_INVALID_METADATA_SERDE_NULL=SerDe is not present in StorageFormat
HIVE_INVALID_METADATA_STORAGE_DESCRIPTOR_MISSING=Table is missing storage descriptor
HIVE_INVALID_METADATA_TABLE_STORAGE_DESCRIPTOR_NULL=Table does not contain a storage descriptor: %s
HIVE_INVALID_METADATA_SERDE_INFO_NULL=Table storage descriptor is missing SerDe info
HIVE_INVALID_METADATA_PARTITION_STORAGE_DESCRIPTOR_NULL=Partition does not contain a storage descriptor: %s
HIVE_INVALID_METADATA_COLUMN_STATISTICS_DATA_INVALID=Invalid column statistics data: %s
HIVE_DATABASE_LOCATION_ERROR_PATH_NOT_SET=Database '%s' location is not set
HIVE_DATABASE_LOCATION_ERROR_PATH_DOES_NOT_EXIST=Database '%s' location does not exist: %s
HIVE_DATABASE_LOCATION_ERROR_LOCATION_NOT_A_DIRECTORY=Database '%s' location is not a directory: %s
HIVE_PATH_ALREADY_EXISTS_UNABLE_TO_RENAME=Unable to rename from %s to %s: target directory already exists
HIVE_PATH_ALREADY_EXISTS_UNABLE_TO_CREATE_DIRECTORY=Unable to create directory %s: target directory already exists
HIVE_PATH_ALREADY_EXISTS_TARGET_DIRECTORY=Target directory for table '%s.%s' already exists: %s
HIVE_PATH_ALREADY_EXISTS_TARGET_DIRECTORY_FOR_NEW_PARTITION=Target directory for new partition '%s' of table '%s.%s' already exists: %s
HIVE_FILESYSTEM_ERROR_FAILED_GETTING_FILESYSTEM=Failed getting FileSystem: %s
HIVE_FILESYSTEM_ERROR_FAILED_DELETE_PARTITION=Failed to delete partition %s files during overwrite
HIVE_FILESYSTEM_ERROR_WHILE_RENAMING_FILE=Error renaming file. fromPath: %s toPath: %s
HIVE_FILESYSTEM_ERROR_FAILED_CHECKING_PATH=Failed checking path: %s
HIVE_FILESYSTEM_ERROR_WHILE_READING=Error reading from %s at position %s
HIVE_FILESYSTEM_ERROR_WHILE_LIST_DIRECTORY=Failed to list directory: %s. %s
HIVE_FILESYSTEM_ERROR_CREATE_DIRECTORY=Failed to create directory: %s
HIVE_FILESYSTEM_ERROR_FAILED_TO_SET_PERMISSION=Failed to set permission on directory: %s
HIVE_FILESYSTEM_ERROR_DELETE_UNPARTITIONED_TABLE=Error deleting from unpartitioned table %s. These items can not be deleted: %s
HIVE_FILESYSTEM_ERROR_FAILED_TO_RENAME=Failed to rename %s to %s: rename returned false
HIVE_SERDE_NOT_FOUND_DESERIALIZER_NOT_FOUND=deserializer does not exist: %s
HIVE_SERDE_NOT_FOUND_SERIALIZER_NOT_FOUND=Serializer does not exist: %s
HIVE_UNSUPPORTED_FORMAT_NOT_A_HIVE_TABLE=Not a Hive table '%s'
HIVE_UNSUPPORTED_FORMAT_SERDE_NOT_SUPPORTED=Output format %s with SerDe %s is not supported
HIVE_UNSUPPORTED_FORMAT_CLASS_NOT_FOUND=Unable to create input format %s
HIVE_UNSUPPORTED_FORMAT_FAILED_TO_LOAD_COMPRESSION_CODEC=Failed to load compression codec: %s
HIVE_UNSUPPORTED_FORMAT_UNKNOWN_S3SELECT_DATATYPE=Attempted to build SQL for unknown S3SelectDataType
HIVE_UNSUPPORTED_FORMAT_UNKNOWN_COMPRESSION_TYPE=Unknown %s compression type %s
HIVE_UNSUPPORTED_FORMAT_FAILED_TO_CREATE_FILESPLIT=Split converter %s failed to create FileSplit.
HIVE_UNSUPPORTED_FORMAT_INPUT_FORMAT_NULL=InputFormat is not present in StorageFormat
HIVE_UNSUPPORTED_FORMAT_OUTPUT_FORMAT_NULL=OutputFormat is not present in StorageFormat
HIVE_UNSUPPORTED_FORMAT_STORAGE_DESCRIPTOR_NULL=Table StorageDescriptor is null for table %s.%s (%s)
HIVE_PARTITION_READ_ONLY_PARTITION_ALREADY_EXISTS=Cannot insert into an existing partition of Hive table: %s
HIVE_PARTITION_READ_ONLY_PARTITION_NULL=partition is null
HIVE_PARTITION_READ_ONLY_UNPARTITIONED_TABLE_INSERT=Cannot insert into bucketed unpartitioned Hive table
HIVE_PARTITION_READ_ONLY_UNPARTITIONED_TABLE_IMMUTABLE=Unpartitioned Hive tables are immutable
HIVE_PARTITION_READ_ONLY_APPEND_TO_EXISTING_PARTITION=Cannot insert into existing partition of bucketed Hive table: %s
HIVE_PARTITION_READ_ONLY_INSERT_TO_EXISTING_PARTITION=Cannot insert into an existing partition of Hive table: %s
HIVE_TOO_MANY_OPEN_PARTITIONS_BUCKET_EXCEEDS_LIMIT=Exceeded limit of %s open writers for partitions/buckets
HIVE_CONCURRENT_MODIFICATION_DETECTED_TABLE_FORMAT_CHANGED=Table format changed during insert
HIVE_CONCURRENT_MODIFICATION_DETECTED_PARTITION_FORMAT_CHANGED=Partition format changed during insert
HIVE_CONCURRENT_MODIFICATION_DETECTED_PARTITION_ADDED_MODIFIED=Partition %s was added or modified during INSERT
HIVE_COLUMN_ORDER_MISMATCH=Partition keys must be the last columns in the table and in the same order as the table properties: %s
HIVE_FILE_MISSING_COLUMN_NAMES_NO_COLUMN_NAME_IN_FOOTER=ORC file does not contain column names in the footer: %s
HIVE_WRITER_OPEN_ERROR_CREATE_FILE=Error creating %s file. %s
HIVE_WRITER_OPEN_ERROR_CREATE_RCFILE=Error creating RCFile file
HIVE_WRITER_OPEN_ERROR_CREATE_PAGEFILE=Error creating pagefile
HIVE_WRITER_OPEN_ERROR_CREATE_EMPTY_FILE=Error creating empty pagefile
HIVE_WRITER_OPEN_ERROR_CREATE_PARQUET_FILE=Error creating Parquet file
HIVE_WRITER_CLOSE_ERROR_ROLLING_BACK_WRITE=Error rolling back write to Hive
HIVE_WRITER_CLOSE_ERROR_WRITE_ZERO_ROW_FILE=Error write zero-row file to Hive
HIVE_WRITER_CLOSE_ERROR_COMMIT_WRITE=Error committing write to Hive. %s
HIVE_WRITER_CLOSE_ERROR_COMMIT_WRITE_TO_HIVE=Error committing write to Hive
HIVE_WRITER_CLOSE_ERROR_COMMIT_WRITE_PARQUET_TO_HIVE=Error committing writing parquet to Hive
HIVE_WRITER_CLOSE_ERROR_ROLLING_BACK_WRITE_PARQUET=Error rolling back write parquet to Hive
HIVE_WRITER_DATA_ERROR_WRITE_TEMPORARY_FILE=Failed to write temporary file: %s
HIVE_WRITER_DATA_ERROR_READ_TEMPORARY_DATA=Failed to read temporary data
HIVE_INVALID_BUCKET_FILES_WRONG_ROW_IN_BUCKET=A row that is supposed to be in bucket %s is encountered. Only rows in bucket %s (modulo %s) are expected
HIVE_INVALID_BUCKET_FILES_CORRUPT_TABLE=Hive table '%s' is corrupt. Found sub-directory in bucket directory for partition: %s
HIVE_INVALID_BUCKET_FILES_FILE_NAME_INVALID=invalid hive bucket file name: %s
HIVE_INVALID_BUCKET_FILES_FILE_DOES_NOT_MATCH_NAMING_PATTERN=Hive table '%s' is corrupt. File '%s' does not match the standard naming pattern, and the number of files in the directory (%s) does not match the declared bucket count (%s) for partition: %s
HIVE_EXCEEDED_PARTITION_LIMIT_QUERY_OVER_TABLE_CAN_READ_MORE=Query over table '%s' can potentially read more than %s partitions
HIVE_PARTITION_DROPPED_DURING_QUERY_PARTITION_NOT_FOUND=Partition no longer exists: %s
HIVE_PARTITION_DROPPED_DURING_QUERY_STATISTICS_RESULT_DOES_NOT_CONTAIN_ENTRY=Statistics result does not contain entry for partition: %s
HIVE_TABLE_DROPPED_DURING_QUERY_DELETE_OPERATION_FAILED=The metastore delete operation failed: %s
HIVE_CORRUPTED_COLUMN_STATISTICS_CORRUPTED_PARTITION_STATISTICS=Corrupted partition statistics (Table: %s Partition: [%s] Column: %s): %s
HIVE_CORRUPTED_COLUMN_STATISTICS_CORRUPTED_PARTITION=Corrupted partition statistics (Table: %s Partition: [%s]): %s
HIVE_CORRUPTED_COLUMN_STATISTICS_CORRUPTED_STATISTICS=Corrupted statistics found when altering partition. Table: %s.%s. Partition: %s
HIVE_EXCEEDED_SPLIT_BUFFERING_LIMIT=Split buffering for %s.%s exceeded memory limit (%s). %s splits are buffered.
HIVE_UNKNOWN_COLUMN_STATISTIC_TYPE=Unknown column statistics type: %s
HIVE_TABLE_BUCKETING_IS_IGNORED_CANNOT_BE_REFERENCED=Table bucketing is ignored. The virtual "$bucket" column cannot be referenced.
HIVE_TRANSACTION_NOT_FOUND=Transaction not found: %s
HIVE_INVALID_ENCRYPTION_METADATA_NO_COLUMN_FOUND=no column found for encryption field %s
HIVE_INVALID_ENCRYPTION_METADATA_SUBFIELD_NOT_FOUND=subfield not found
HIVE_INVALID_ENCRYPTION_METADATA_DWRF_ENCRYPTION_NOT_SET=Both %s and %s need to be set for DWRF encryption
HIVE_INVALID_ENCRYPTION_METADATA_UNKNOW_DATA=Unknown encryptionMetadata type: %s
HIVE_INVALID_ENCRYPTION_METADATA_BOTH_TABLE_AND_COLUMN_SETTINGS_PRESENT=Exactly one of table or column settings can be present. Both are present
HIVE_INVALID_ENCRYPTION_METADATA_NEITHER_TABLE_NOR_COLUMN_SETTINGS_PRESENT=Exactly one of table or column settings can be present. None are present
HIVE_UNSUPPORTED_ENCRYPTION_OPERATION_CREATE_ENCRYPTED_TABLE_WITHOUT_PARTITION=Creating an encrypted table without partitions is not supported. Use CREATE TABLE AS SELECT to create an encrypted table without partitions
HIVE_UNSUPPORTED_ENCRYPTION_OPERATION_INSERT_INTO_EXISTING_TABLE=Inserting into an existing table with encryption enabled is not supported yet
HIVE_UNSUPPORTED_ENCRYPTION_OPERATION_INSERT_INTO_EXISTING_PARTITION=Inserting into an existing partition with encryption enabled is not supported yet
MALFORMED_HIVE_FILE_STATISTICS_INVALID_POSITION=Invalid position: %d specified for FileStatistics page
MALFORMED_HIVE_FILE_STATISTICS_CREATION_FOR_PARTITION_NOT_EQUAL_TO_FILESIZE_COUNT=During manifest creation for partition= %s, filename count= %s is not equal to filesizes count= %s
MALFORMED_HIVE_FILE_STATISTICS_FAILED_DE-COMPRESSING_FILE_NAME=Failed de-compressing the file names in manifest
MALFORMED_HIVE_FILE_STATISTICS_FAILED_COMPRESSING_FILE_SIZE=Failed compressing the file sizes for manifest
MALFORMED_HIVE_FILE_STATISTICS_FAILED_DE-COMPRESSING_FILE_SIZE=Failed de-compressing the file sizes in manifest
MALFORMED_HIVE_FILE_STATISTICS_FILE_NOT_STORED_IN_MANFEST=Filename = %s not stored in manifest. Partition = %s, TableName = %s
MALFORMED_HIVE_FILE_STATISTICS_FILE_SIZE_MISMATCH=FilesizeFromManifest = %s is not equal to FilesizeFromStorage = %s. File = %s, Partition = %s, TableName = %s
MALFORMED_HIVE_FILE_STATISTICS_NUMBER_OF_FILES_MISMATCH=Number of files in Manifest = %s is not equal to Number of files in storage = %s. Partition = %s, TableName = %s
HIVE_INVALID_FILE_NAMES_HIVE_TABLE_CORRUPTED=Hive table '%s' is corrupt. Some of the filenames in the partition: %s are not integers
HIVE_CORRUPTED_PARTITION_CACHE_PARTITION_MISMATCH=Partition returned from cache is different from partition from Metastore.%nPartition name = %s.%nPartition from cache = %s%n Partition from Metastore = %s
HIVE_RANGER_SERVER_ERROR=Unable to fetch policy information from ranger
HIVE_FUNCTION_UNSUPPORTED_HIVE_TYPE=Unsupported Hive type %s
HIVE_FUNCTION_UNSUPPORTED_PRESTO_TYPE=Unsupported Presto type %s
HIVE_FUNCTION_UNSUPPORTED_FUNCTION_TYPE=Unsupported function type %s / %s
HUDI_UNKNOWN_TABLE_TYPE=Unknown table type %s
HUDI_INVALID_METADATA_TABLE_NOT_FOUND=Table %s.%s expected but not found
HUDI_INVALID_METADATA_PARTITION_NOT_FOUND=Partition %s expected but not found
HUDI_INVALID_PARTITION_VALUE_FOR_PARTITION_KEY=Invalid partition value '%s' for %s partition key: %s
HUDI_FILESYSTEM_ERROR_COULD_NOT_OPEN_FILE=Could not open file system for %s
HUDI_CANNOT_OPEN_SPLIT_INVALID_SPLIT=Split without base file is invalid
HUDI_CANNOT_OPEN_SPLIT_CANNOT_OPEN_HUDI_SPLIT=Error opening Hudi split %s (offset=%s, length=%s): %s
HUDI_CANNOT_OPEN_SPLIT_CANNOT_OPEN_HIVE_SPLIT=Error opening Hive split %s using %s: %s
HUDI_CANNOT_OPEN_SPLIT_CANNOT_CREATE_INPUT_FORMAT=Unable to create input format %s
HUDI_CANNOT_GENERATE_SPLIT=Error generating Hudi split
ICEBERG_INVALID_METADATA_LOCATION_NULL=Table is missing [%s] property: %s
ICEBERG_INVALID_METADATA_SNAPSHOT_NULL=Snapshot ID [%s] does not exist for table: %s
ICEBERG_INVALID_METADATA_FOUND_MORE_THAN_ONE_FIELD_FOR_BLOB=blob metadata for blob type %s in statistics file %s must contain only one field. Found %d fields
ICEBERG_INVALID_METADATA_DUPLICATE_FILES=able '%s' has duplicate statistics files '%s' and '%s' for snapshot ID %s
ICEBERG_TOO_MANY_OPEN_PARTITIONS_EXCEEDS_LIMIT=Exceeded limit of %s open writers for partitions
ICEBERG_INVALID_PARTITION_VALUE=Invalid partition value '%s' for %s partition key: %s
ICEBERG_BAD_DATA_ERROR_OPENING_ICEBERG_SPLIT=Error opening Iceberg split %s (offset=%s, length=%s): %s
ICEBERG_MISSING_DATA_ERROR_OPENING_ICEBERG_SPLIT=Error opening Iceberg split %s (offset=%s, length=%s): %s
ICEBERG_CANNOT_OPEN_SPLIT=Error opening Iceberg split %s (offset=%s, length=%s): %s
ICEBERG_WRITER_OPEN_ERROR_WHILE_CREATION_PARQUET=Error creating Parquet file
ICEBERG_WRITER_OPEN_ERROR_WHILE_CREATION_ORC=Error creating ORC file
ICEBERG_FILESYSTEM_ERROR_FAILED_DELETE_FILE=Failed to delete file: %s
ICEBERG_FILESYSTEM_ERROR_FAILED_CREATE_INPUT_FILE=Failed to create input file: %s
ICEBERG_FILESYSTEM_ERROR_FAILED_CREATE_OUTPUT=Failed to create output file: %s
ICEBERG_INVALID_SNAPSHOT_ID_FOR_TABLE=Invalid snapshot [%s] for table: %s
JDBC_ERROR_FAILED_TO_FIND_SCHEMA_NAME=Failed to find remote schema name: %s
JDBC_ERROR_FAILED_TO_FIND_TABLE_NAME=Failed to find remote table name: %s
KAFKA_SPLIT_ERROR_CANNOT_READ_DATA=Cannot read data from topic '%s', partition '%s', startOffset %s, endOffset %s, leader %s
KAFKA_SPLIT_ERROR_CANNOT_LIST_SPLIT=Cannot list splits for table '%s' reading topic '%s'
KAFKA_CONSUMER_ERROR_FAILED_TO_FIND_OFFSET=Failed to find offset by timestamp: %d for partition %d
KAFKA_SCHEMA_ERROR_UNABLE_READ_DATA_SCHEMA=Unable to read data schema at '%s'
KAFKA_PRODUCER_ERROR_FAILED_TO_SEND=%d producer record('s) failed to send
NOT_PERMITTED_SCHEMA_NAME_DOES_NOT_EXIT=Schema is required to list tables
NOT_PERMITTED_ILLEGAL_OPERATION=User '%s' is not permitted to perform '%s' on schema '%s'
LARK_API_ERROR_ILLEGAL_RESPONSE=Illegal response data of sheet %s @ %s
LARK_API_ERROR_BAD_RESPONSE=Bad response: [%d] %s
SCHEMA_TOKEN_NOT_PROVIDED=Schema token is required but not provided
SCHEMA_ALREADY_EXISTS=Schema '%s' already exists or created by others
SCHEMA_NOT_EXISTS_OR_NOT_VISIBLE=Schema %s not exists or not visible
SCHEMA_NOT_EXISTS_SCHEMA_NOT_FOUND=Schema '%s' does not exist
SCHEMA_NOT_READABLE=Spreadsheet %s not readable
SHEET_NAME_AMBIGUOUS=Ambiguous name %s in spreadsheet %s: matched sheets: %s
SHEET_INVALID_HEADER_DUPLICATE_NAME=Duplicated name %s in Column#%s and Column#%s
SHEET_BAD_DATA_SHEET_EMPTY=Sheet %s.%s is empty
LOCAL_FILE_NO_FILES_NO_FOUND=No matching files found in directory: %s
LOCAL_FILE_FILESYSTEM_ERROR_WHILE_LISTING_FILE=Error listing files in directory: %s
LOCAL_FILE_READ_ERROR=Error reading file: %s
MISSING_DATA_TABLE_ID_NOT_FOUND=Failed to find table on a worker.
MISSING_DATA_ROW_MISMATCH=Expected to find [%s] rows on a worker, but found [%s].
MEMORY_LIMIT_EXCEEDED_FOR_MEMORY_CONNECTOR=Memory limit [%d] for memory connector exceeded
OPEN_TELEMETRY_CONTEXT_PROPAGATOR_ERROR_UNSUPPORTED_PROPOGATOR=Only b3 single header context propagation mode is currently supported.
PARQUET_UNSUPPORTED_COLUMN_TYPE=Column: %s, Encoding: %s
PARQUET_UNSUPPORTED_ENCODING_DELTA_BYTE_ARRAY_DELTA_LENGTH_BYTE_ARRAY=Column: %s, Encoding: %s
PARQUET_UNSUPPORTED_ENCODING_DICTIONARY_ENCODING=Dictionary encoding is not supported: %s
PARQUET_IO_READ_ERROR_PARQUET_COLUMN=Error reading Parquet column %s
PARQUET_IO_READ_ERROR_PARQUET_PAGE_IN_COLUMN=Error reading parquet page %s in column %s
PARQUET_INCORRECT_DECODING_WRONG_COUNT=Wrong count: ex=%d, act=%d, col=%s, file=%s
PARQUET_INCORRECT_DECODING_WRONG_VALUE=Wrong value: pos=%d, ex=%s, act=%s, col=%s-%s, file=%s
PARQUET_INCORRECT_DECODING_WRONG_RL_COUNT=Wrong RL count: ex=%s, act=%s, col=%s-%s, file=%s
PARQUET_INCORRECT_DECODING_WRONG_RL_VALUE=Wrong RL value: pos=%d, ex=%s, act=%s, col=%s-%s, file=%s
PARQUET_INCORRECT_DECODING_WRONG_DL_COUNT=Wrong DL count: ex=%s, act=%s, col=%s-%s, file=%s
PINOT_UNSUPPORTED_COLUMN_TYPE_FAILED_TO_COLUMN_JAVATYPE=Failed to write column %s. pinotColumnType %s, javaType %s
PINOT_UNSUPPORTED_COLUMN_TYPE_FAILED_TO_COLUMN_PRESTOTYPE=Failed to write column %s. pinotColumnType %s, prestoType %s
PROMETHEUS_UNKNOWN_ERROR_READ_METRICS=Error reading metrics
PROMETHEUS_UNKNOWN_ERROR_BAD_RESPONSE=Bad response %s %s
PROMETHEUS_UNKNOWN_ERROR_FAILED_TO_READ_BEARER_TOKEN=Failed to read bearer token file: %s
PROMETHEUS_UNKNOWN_ERROR_SPLIT_URI_INVALID=split URI invalid: %s
PROMETHEUS_UNKNOWN_ERROR_DECIMAL_EPOCH_TO_SQL_TIMESTAMP_NOT_POSSIBLE=unable to deserialize timestamp: %s
PROMETHEUS_TABLES_METRICS_RETRIEVE_ERROR_NO_METRIC_LIST_RETURNED=Prometheus did not return metrics list (table names): %s
PROMETHEUS_PARSE_ERROR_UNABLE_TO_PARSE_RESPONSE=Unable to parse Prometheus response: %s %s %s
PROMETHEUS_OUTPUT_ERROR_UNABLE_TO_HANDLE_RESULT=Unable to handle Prometheus result: %s
SHEETS_UNKNOWN_TABLE_ERROR_SHEET_EXPRESSION_NOT_FOUND=Sheet expression not found for table %s
SHEETS_UNKNOWN_TABLE_ERROR_FAILED_READING_DATA=Failed reading data from sheet: %s
SHEETS_UNKNOWN_TABLE_ERROR_METADATA_NOT_FOUND=Metadata not found for table %s
SHEETS_TABLE_LOAD_ERROR=Error loading data for table: %s
UNSUPPORTED_STORAGE_TYPE_DOES_NOT_SUPPORT_REMOTE_ACCESS=Configured TempStorage does not support remote access required for distributing broadcast tables.
STORAGE_ERROR_UNABLE_TO_DELET_FILE=Unable to delete broadcast spill file
STORAGE_ERROR_CHECKSUM_DOES_NOT_MATCH=Disk page checksum does not match. Data seems to be corrupted on disk for file %s
STORAGE_ERROR_UNABLE_TO_READ_DATA=Unable to read data from disk:
MALFORMED_QUERY_FILE_SQL_FILE_SIZE_MISMATCH=sql file size %s is different from expected sqlFileSizeInBytes %s
MALFORMED_QUERY_FILE_HASH_CODE_MISMATCH=actual hash code %s is different from expected sqlFileHexHash %s
THRIFT_SERVICE_CONNECTION_ERROR_WITH_THRIFT_SERVER=Error communicating with remote Thrift server
THRIFT_SERVICE_INVALID_RESPONSE_TABLE_NAME_MISMATCH=Requested and actual table names are different
THRIFT_SERVICE_GENERIC_REMOTE_ERROR=Exception raised by remote Thrift server: %s
