services:
  spark-master:
    image: spark:3.5.5-java17
    environment:
      - "TZ=America/Bahia_Banderas"
      - "SPARK_LOCAL_IP=spark-master"
      - "SPARK_NO_DAEMONIZE=true"
    depends_on:
      - hadoop-master
    command: /opt/spark/sbin/start-master.sh
    volumes:
      - ./work:/spark/work
  spark-worker:
    image: spark:3.5.5-java17
    depends_on:
      - spark-master
    command: /opt/spark/sbin/start-worker.sh spark://spark-master:7077
    environment:
      - "TZ=America/Bahia_Banderas"
      - SPARK_DRIVER_MEMORY=1G
      - SPARK_WORKER_CORES=4
      - SPARK_WORKER_MEMORY=1G
      # We need to clean up app work directories aggressively because CI
      # machines do not have much disk space. To prevent running out of space,
      # We add the below options to tell the worker to quickly clean up
      # completed app work directories
      - "SPARK_WORKER_OPTS=-Dspark.worker.cleanup.enabled=true -Dspark.worker.cleanup.interval=5 -Dspark.worker.cleanup.appDataTtl=10"
      - SPARK_NO_DAEMONIZE=true
    volumes:
      - ./work:/spark/work
  spark-submit:
    image: spark:3.5.5-java17
    environment:
      - "TZ=America/Bahia_Banderas"
  hadoop-master:
    image: prestodb/hdp2.6-hive:10
    hostname: hadoop-master
    environment:
      - "TZ=America/Bahia_Banderas"
    ports:
      # metastore
      - '9083:9083'
      # namenode
      - '9000:9000'
      - '8020:8020'
      # datanode
      - '50010:50010'
